{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3ae285-386f-4e88-b785-787e63b1f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python-headless in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python --user\n",
    "%pip install opencv-python-headless\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d27b2f0-d2af-42bf-b995-7039abfb8f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64311302-0100-45f6-8838-d4a61053e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741270696.886746 2568791 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n"
     ]
    }
   ],
   "source": [
    "# MEDIAPIPE POSE ESTIMATION SETUP\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Initialize MediaPipe pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,  # Set to False for video\n",
    "    model_complexity=2,      # Higher accuracy\n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da59a9bf-10f7-4aaa-a759-a8f6be2d6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path):\n",
    "    \"\"\"Process an image and extract pose landmarks\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    return image, image_rgb, results\n",
    "\n",
    "def visualize_pose(image_rgb, results):\n",
    "    \"\"\"Visualize detected pose landmarks on an image\"\"\"\n",
    "    annotated_image = image_rgb.copy()\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS\n",
    "    )\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.title(\"Detected Pose\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b35c48-dbcf-4dc0-b2cc-ab8f90613fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BODY PART MAPPING\n",
    "\n",
    "# Define constants for pose comparison\n",
    "POSE_LANDMARKS = {\n",
    "    'left_wrist': mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "    'right_wrist': mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "    'left_elbow': mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "    'right_elbow': mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "    'left_shoulder': mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "    'right_shoulder': mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "    'left_hip': mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "    'right_hip': mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "    'left_knee': mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "    'right_knee': mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "    'left_ankle': mp_pose.PoseLandmark.LEFT_ANKLE.value,\n",
    "    'right_ankle': mp_pose.PoseLandmark.RIGHT_ANKLE.value,\n",
    "    'left_foot_index': mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value,\n",
    "    'right_foot_index': mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value,\n",
    "}\n",
    "\n",
    "# Define groups for specific analysis\n",
    "ARM_LANDMARKS = [\n",
    "    'left_wrist', 'right_wrist', 'left_elbow', 'right_elbow', \n",
    "    'left_shoulder', 'right_shoulder'\n",
    "]\n",
    "\n",
    "FOOT_LANDMARKS = [\n",
    "    'left_hip', 'right_hip', 'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle', 'left_foot_index', 'right_foot_index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f646aa57-07db-42f0-82bd-c5f17e802a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE POSE MANAGER CLASS\n",
    "class ReferencePoseManager:\n",
    "    \"\"\"Manages reference poses for Spanish dance movements\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"spanish_dance_references\"):\n",
    "        \"\"\"Initialize the reference pose manager\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.reference_poses = {}\n",
    "        self.ensure_data_dir()\n",
    "        \n",
    "    def ensure_data_dir(self):\n",
    "        \"\"\"Ensure the data directory exists\"\"\"\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "            \n",
    "    def add_reference_pose(self, pose_name, image_path, pose_type=\"arm\", description=\"\"):\n",
    "        \"\"\"Add a reference pose to the database\"\"\"\n",
    "        image, _, results = process_image(image_path)\n",
    "        \n",
    "        if not results.pose_landmarks:\n",
    "            print(f\"No pose detected in {image_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Extract landmarks as normalized vectors\n",
    "        landmarks = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        \n",
    "        self.reference_poses[pose_name] = {\n",
    "            'landmarks': landmarks,\n",
    "            'type': pose_type,  # 'arm' or 'foot'\n",
    "            'description': description,\n",
    "            'image_path': image_path\n",
    "        }\n",
    "        \n",
    "        # Save to disk\n",
    "        self.save_references()\n",
    "        return True\n",
    "    \n",
    "    def save_references(self):\n",
    "        \"\"\"Save reference poses to disk\"\"\"\n",
    "        with open(os.path.join(self.data_dir, \"reference_poses.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.reference_poses, f)\n",
    "    \n",
    "    def load_references(self):\n",
    "        \"\"\"Load reference poses from disk\"\"\"\n",
    "        ref_file = os.path.join(self.data_dir, \"reference_poses.pkl\")\n",
    "        if os.path.exists(ref_file):\n",
    "            with open(ref_file, \"rb\") as f:\n",
    "                self.reference_poses = pickle.load(f)\n",
    "    \n",
    "    def visualize_reference_pose(self, pose_name):\n",
    "        \"\"\"Visualize a reference pose\"\"\"\n",
    "        if pose_name not in self.reference_poses:\n",
    "            print(f\"Reference pose '{pose_name}' not found\")\n",
    "            return\n",
    "        \n",
    "        ref_data = self.reference_poses[pose_name]\n",
    "        image, image_rgb, results = process_image(ref_data['image_path'])\n",
    "        \n",
    "        print(f\"Reference Pose: {pose_name}\")\n",
    "        print(f\"Type: {ref_data['type']}\")\n",
    "        print(f\"Description: {ref_data['description']}\")\n",
    "        \n",
    "        visualize_pose(image_rgb, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281cee77-4a63-4fe0-974c-6b7fe0954a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSE ANALYZER CLASS\n",
    "class PoseAnalyzer:\n",
    "    \"\"\"Analyzes dancer poses against reference poses\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_manager):\n",
    "        \"\"\"Initialize the pose analyzer\"\"\"\n",
    "        self.ref_manager = reference_manager\n",
    "        \n",
    "    def normalize_pose(self, landmarks):\n",
    "        \"\"\"Normalize pose by centering and scaling\"\"\"\n",
    "        # Extract x, y coordinates\n",
    "        coords = np.array([[lm.x, lm.y] for lm in landmarks])\n",
    "        \n",
    "        # Find the bounding box\n",
    "        min_x, min_y = np.min(coords, axis=0)\n",
    "        max_x, max_y = np.max(coords, axis=0)\n",
    "        \n",
    "        # Scale and center\n",
    "        scale_x = 1.0 / (max_x - min_x) if max_x > min_x else 1.0\n",
    "        scale_y = 1.0 / (max_y - min_y) if max_y > min_y else 1.0\n",
    "        \n",
    "        normalized = []\n",
    "        for lm in landmarks:\n",
    "            normalized_lm = mp_pose.PoseLandmark()\n",
    "            normalized_lm.x = (lm.x - min_x) * scale_x\n",
    "            normalized_lm.y = (lm.y - min_y) * scale_y\n",
    "            normalized_lm.z = lm.z  # Keep z as is\n",
    "            normalized_lm.visibility = lm.visibility\n",
    "            normalized.append(normalized_lm)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def extract_landmark_vectors(self, landmarks, landmark_group):\n",
    "        \"\"\"Extract vectors for specific landmark groups (arm or foot)\"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        if landmark_group == \"arm\":\n",
    "            target_landmarks = ARM_LANDMARKS\n",
    "        elif landmark_group == \"foot\":\n",
    "            target_landmarks = FOOT_LANDMARKS\n",
    "        else:\n",
    "            # Use all landmarks\n",
    "            return np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten().reshape(1, -1)\n",
    "        \n",
    "        # Extract coordinates for target landmarks\n",
    "        coords = []\n",
    "        for name in target_landmarks:\n",
    "            idx = POSE_LANDMARKS[name]\n",
    "            lm = landmarks[idx]\n",
    "            coords.extend([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        return np.array(coords).reshape(1, -1)\n",
    "    \n",
    "    def compare_poses(self, dancer_landmarks, ref_pose_name, view=\"front\"):\n",
    "        \"\"\"Compare dancer pose to a reference pose\"\"\"\n",
    "        if ref_pose_name not in self.ref_manager.reference_poses:\n",
    "            return {\"score\": 0, \"message\": f\"Reference pose '{ref_pose_name}' not found\"}\n",
    "        \n",
    "        ref_data = self.ref_manager.reference_poses[ref_pose_name]\n",
    "        \n",
    "        # Determine which landmark group to use based on view and pose type\n",
    "        landmark_group = None\n",
    "        if view == \"front\" and ref_data['type'] == 'arm':\n",
    "            landmark_group = \"arm\"\n",
    "        elif view == \"side\" and ref_data['type'] == 'foot':\n",
    "            landmark_group = \"foot\"\n",
    "        \n",
    "        # Convert reference landmarks format\n",
    "        ref_landmarks = []\n",
    "        for lm_data in ref_data['landmarks']:\n",
    "            lm = mp_pose.PoseLandmark()\n",
    "            lm.x, lm.y, lm.z, lm.visibility = lm_data\n",
    "            ref_landmarks.append(lm)\n",
    "        \n",
    "        # Normalize both poses\n",
    "        norm_dancer = self.normalize_pose(dancer_landmarks)\n",
    "        norm_ref = self.normalize_pose(ref_landmarks)\n",
    "        \n",
    "        # Extract vectors for comparison\n",
    "        dancer_vector = self.extract_landmark_vectors(norm_dancer, landmark_group)\n",
    "        ref_vector = self.extract_landmark_vectors(norm_ref, landmark_group)\n",
    "        \n",
    "        # Calculate similarity score (cosine similarity)\n",
    "        similarity = cosine_similarity(dancer_vector, ref_vector)[0][0]\n",
    "        \n",
    "        # Scale to 0-100%\n",
    "        score = max(0, min(100, (similarity + 1) * 50))\n",
    "        \n",
    "        # Generate feedback message\n",
    "        message = self._generate_feedback(score, ref_pose_name, ref_data)\n",
    "        \n",
    "        return {\n",
    "            \"score\": score,\n",
    "            \"message\": message\n",
    "        }\n",
    "    \n",
    "    def _generate_feedback(self, score, pose_name, ref_data):\n",
    "        \"\"\"Generate feedback based on score\"\"\"\n",
    "        if score >= 90:\n",
    "            return f\"Excellent execution of {pose_name}! {ref_data['description']}\"\n",
    "        elif score >= 75:\n",
    "            return f\"Good execution of {pose_name}. Minor adjustments needed. {ref_data['description']}\"\n",
    "        elif score >= 60:\n",
    "            return f\"Fair execution of {pose_name}. Review the position details. {ref_data['description']}\"\n",
    "        else:\n",
    "            return f\"Needs improvement on {pose_name}. Focus on the key elements: {ref_data['description']}\"\n",
    "    \n",
    "    def analyze_dance_sequence(self, dancer_image_path, reference_sequence, view=\"front\"):\n",
    "        \"\"\"Analyze a dancer's execution of a sequence of poses\"\"\"\n",
    "        dancer_image, dancer_rgb, dancer_results = process_image(dancer_image_path)\n",
    "        \n",
    "        if not dancer_results.pose_landmarks:\n",
    "            return {\"error\": \"No pose detected in dancer image\"}\n",
    "        \n",
    "        # Visualize the detected pose\n",
    "        visualize_pose(dancer_rgb, dancer_results)\n",
    "        \n",
    "        # Compare with each reference pose in the sequence\n",
    "        results = {}\n",
    "        for pose_name in reference_sequence:\n",
    "            results[pose_name] = self.compare_poses(\n",
    "                dancer_results.pose_landmarks.landmark, \n",
    "                pose_name, \n",
    "                view\n",
    "            )\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3d515e-5025-4a37-a019-5e69d85eea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANCE EVALUATOR CLASS\n",
    "class DanceEvaluator:\n",
    "    \"\"\"Evaluates a complete dance performance and generates reports\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        \"\"\"Initialize the dance evaluator\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        \n",
    "    def evaluate_performance(self, dancer_frontal_image, dancer_side_image, arm_sequence, foot_sequence):\n",
    "        \"\"\"Evaluate a dance performance from both frontal and side views\"\"\"\n",
    "        # Analyze arm movements (frontal view)\n",
    "        print(\"Analyzing arm movements (frontal view)...\")\n",
    "        arm_results = self.analyzer.analyze_dance_sequence(\n",
    "            dancer_frontal_image, \n",
    "            arm_sequence, \n",
    "            view=\"front\"\n",
    "        )\n",
    "        \n",
    "        # Analyze footwork (side view)\n",
    "        print(\"Analyzing footwork (side view)...\")\n",
    "        foot_results = self.analyzer.analyze_dance_sequence(\n",
    "            dancer_side_image, \n",
    "            foot_sequence, \n",
    "            view=\"side\"\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        complete_evaluation = {\n",
    "            \"arm_movements\": arm_results,\n",
    "            \"footwork\": foot_results\n",
    "        }\n",
    "        \n",
    "        return complete_evaluation\n",
    "    \n",
    "    def generate_feedback_report(self, evaluation_results):\n",
    "        \"\"\"Generate a comprehensive feedback report\"\"\"\n",
    "        arm_results = evaluation_results.get(\"arm_movements\", {})\n",
    "        foot_results = evaluation_results.get(\"footwork\", {})\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        all_scores = []\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_scores.append(result[\"score\"])\n",
    "        \n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_scores.append(result[\"score\"])\n",
    "        \n",
    "        overall_score = np.mean(all_scores) if all_scores else 0\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html = f\"\"\"\n",
    "        <h2>Spanish Dance Performance Evaluation</h2>\n",
    "        <h3>Overall Score: {overall_score:.1f}%</h3>\n",
    "        \n",
    "        <h3>Arm Movements (Frontal View)</h3>\n",
    "        <table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Position</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Score</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Feedback</th>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add arm movement results\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                score = result[\"score\"]\n",
    "                message = result.get(\"message\", \"\")\n",
    "                \n",
    "                # Determine rating color based on score\n",
    "                if score >= 90:\n",
    "                    color = \"green\"\n",
    "                    rating = \"Excellent\"\n",
    "                elif score >= 75:\n",
    "                    color = \"#5cb85c\"  # lighter green\n",
    "                    rating = \"Good\"\n",
    "                elif score >= 60:\n",
    "                    color = \"orange\"\n",
    "                    rating = \"Fair\"\n",
    "                else:\n",
    "                    color = \"red\"\n",
    "                    rating = \"Needs Improvement\"\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\">{pose_name}</td>\n",
    "                    <td style=\"padding: 8px; color: {color};\">{score:.1f}% ({rating})</td>\n",
    "                    <td style=\"padding: 8px;\">{message}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Footwork (Side View)</h3>\n",
    "        <table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Position</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Score</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Feedback</th>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add footwork results\n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                score = result[\"score\"]\n",
    "                message = result.get(\"message\", \"\")\n",
    "                \n",
    "                # Determine rating color based on score\n",
    "                if score >= 90:\n",
    "                    color = \"green\"\n",
    "                    rating = \"Excellent\"\n",
    "                elif score >= 75:\n",
    "                    color = \"#5cb85c\"  # lighter green\n",
    "                    rating = \"Good\"\n",
    "                elif score >= 60:\n",
    "                    color = \"orange\"\n",
    "                    rating = \"Fair\"\n",
    "                else:\n",
    "                    color = \"red\"\n",
    "                    rating = \"Needs Improvement\"\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\">{pose_name}</td>\n",
    "                    <td style=\"padding: 8px; color: {color};\">{score:.1f}% ({rating})</td>\n",
    "                    <td style=\"padding: 8px;\">{message}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Areas for Improvement</h3>\n",
    "        <p>Focus on the following positions:</p>\n",
    "        <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find lowest-scoring positions for improvement focus\n",
    "        all_results = []\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_results.append((pose_name, result[\"score\"], \"arm\"))\n",
    "        \n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_results.append((pose_name, result[\"score\"], \"foot\"))\n",
    "        \n",
    "        # Sort by score (ascending) and take the 3 lowest\n",
    "        all_results.sort(key=lambda x: x[1])\n",
    "        for pose_name, score, pose_type in all_results[:3]:\n",
    "            html += f\"<li><strong>{pose_name}</strong> ({pose_type} position): {score:.1f}%</li>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </ul>\n",
    "        \n",
    "        <p>Practice these positions to improve your overall performance.</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Display the HTML report\n",
    "        display(HTML(html))\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def display_report(self, evaluation_results):\n",
    "        \"\"\"Display the feedback report\"\"\"\n",
    "        return self.generate_feedback_report(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86915d53-946b-44f6-8a11-6abc9e1e9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USAGE\n",
    "# Create the reference pose manager and add sample reference poses\n",
    "def setup_reference_dataset():\n",
    "    \"\"\"Set up a sample reference dataset\"\"\"\n",
    "    ref_manager = ReferencePoseManager(data_dir=\"spanish_dance_references\")\n",
    "    \n",
    "    # Example: Add reference poses for arm movements (frontal view)\n",
    "    arm_poses = {\n",
    "        \"brazos_en_quinta\": {\n",
    "            \"path\": \"dataset/arms/quinta.jpg\",\n",
    "            \"description\": \"Arms in fifth position, rounded above the head\"\n",
    "        },\n",
    "        \"braceo_flamenco\": {\n",
    "            \"path\": \"dataset/arms/braceo.jpg\",\n",
    "            \"description\": \"Flowing arm movement with wrists rotating\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, data in arm_poses.items():\n",
    "        ref_manager.add_reference_pose(name, data[\"path\"], \"arm\", data[\"description\"])\n",
    "    \n",
    "    # Example: Add reference poses for footwork (side view)\n",
    "    foot_poses = {\n",
    "        \"planta_tacón\": {\n",
    "            \"path\": \"dataset/feet/planta_tacon.jpg\",\n",
    "            \"description\": \"Heel-to-toe rhythmic step\"\n",
    "        },\n",
    "        \"golpe\": {\n",
    "            \"path\": \"dataset/feet/golpe.jpg\",\n",
    "            \"description\": \"Sharp stamp of the whole foot\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, data in foot_poses.items():\n",
    "        ref_manager.add_reference_pose(name, data[\"path\"], \"foot\", data[\"description\"])\n",
    "    \n",
    "    return ref_manager\n",
    "\n",
    "# Cell 8: Evaluate a dancer's performance\n",
    "def evaluate_dancer_performance(dancer_name, frontal_image_path, side_image_path):\n",
    "    \"\"\"Evaluate a dancer's performance\"\"\"\n",
    "    # Load the reference dataset\n",
    "    ref_manager = ReferencePoseManager(data_dir=\"spanish_dance_references\")\n",
    "    ref_manager.load_references()\n",
    "    \n",
    "    # Create the analyzer and evaluator\n",
    "    analyzer = PoseAnalyzer(ref_manager)\n",
    "    evaluator = DanceEvaluator(analyzer)\n",
    "    \n",
    "    # Define the sequence of positions to evaluate\n",
    "    arm_sequence = [\"brazos_en_quinta\", \"braceo_flamenco\"]\n",
    "    foot_sequence = [\"planta_tacón\", \"golpe\"]\n",
    "    \n",
    "    # Evaluate the dancer's performance\n",
    "    results = evaluator.evaluate_performance(\n",
    "        frontal_image_path,\n",
    "        side_image_path,\n",
    "        arm_sequence,\n",
    "        foot_sequence\n",
    "    )\n",
    "    \n",
    "    # Display the evaluation report\n",
    "    print(f\"Performance Evaluation for {dancer_name}\")\n",
    "    evaluator.display_report(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cell 9: Run evaluation on test data\n",
    "# Uncomment and run when ready\n",
    "# setup_reference_dataset()\n",
    "# evaluate_dancer_performance(\n",
    "#     \"Test Dancer\",\n",
    "#     \"test_data/dancer_front.jpg\",\n",
    "#     \"test_data/dancer_side.jpg\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9277c-3ccd-4032-abb2-73bcbfd4250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741270696.943663 2573922 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1741270696.946025 2568791 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n",
      "W0000 00:00:1741270696.984349 2573921 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741270697.000711 2573940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741270697.015037 2573939 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-03-06 15:18:53.383 python[44073:2568791] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-06 15:18:53.383 python[44073:2568791] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "#SPANISH DANCE POSE ANALYSIS TOOL\n",
    "\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Spanish Dance Pose Analysis Tool\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "# Function to load predefined images\n",
    "def load_predefined_images():\n",
    "    global predefined_images\n",
    "    file_paths = filedialog.askopenfilenames(title=\"Select Predefined Position Images\", filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")])\n",
    "    predefined_images = [cv2.imread(file) for file in file_paths]\n",
    "    messagebox.showinfo(\"Info\", f\"Loaded {len(predefined_images)} predefined images\")\n",
    "\n",
    "# Function to load exercise video\n",
    "def load_exercise_video():\n",
    "    global exercise_video_path\n",
    "    exercise_video_path = filedialog.askopenfilename(title=\"Select Exercise Video\", filetypes=[(\"Video Files\", \"*.mp4;*.avi;*.mov\")])\n",
    "    messagebox.showinfo(\"Info\", \"Exercise video loaded successfully\")\n",
    "\n",
    "# Function to start real-time camera observation\n",
    "def start_camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame using MediaPipe Pose\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                h, w, _ = frame.shape\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "        \n",
    "        cv2.imshow(\"Real-Time Pose Tracking\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to generate checklist and report\n",
    "def generate_report():\n",
    "    # Placeholder logic for evaluation\n",
    "    messagebox.showinfo(\"Report\", \"Checklist and report generated successfully!\")\n",
    "\n",
    "# GUI Buttons\n",
    "btn_load_images = tk.Button(root, text=\"Load Predefined Images\", command=load_predefined_images)\n",
    "btn_load_images.pack(pady=10)\n",
    "\n",
    "btn_load_video = tk.Button(root, text=\"Load Exercise Video\", command=load_exercise_video)\n",
    "btn_load_video.pack(pady=10)\n",
    "\n",
    "btn_start_camera = tk.Button(root, text=\"Start Camera Observation\", command=start_camera)\n",
    "btn_start_camera.pack(pady=10)\n",
    "\n",
    "btn_generate_report = tk.Button(root, text=\"Generate Report\", command=generate_report)\n",
    "btn_generate_report.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
