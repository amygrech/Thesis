{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3ae285-386f-4e88-b785-787e63b1f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python-headless in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python --user\n",
    "%pip install opencv-python-headless\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d27b2f0-d2af-42bf-b995-7039abfb8f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de146a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: numpy in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: tk in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: pillow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (11.2.1)\n",
      "Requirement already satisfied: roboflow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.1.61)\n",
      "Requirement already satisfied: tensorflow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2025.1.31)\n",
      "Requirement already satisfied: idna==3.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pillow-heif>=0.18.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.22.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: requests in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.72.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests->roboflow) (3.4.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.10.0\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall opencv-python-headless 4.10.0\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for opencv-python-headless.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: The package was installed by conda. You should check if it can uninstall the package.\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe numpy scikit-learn matplotlib tk pillow roboflow tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95115b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.72.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/amygrech/.local/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9577717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.1.61)\n",
      "Requirement already satisfied: certifi in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2025.1.31)\n",
      "Requirement already satisfied: idna==3.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.26.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (11.2.1)\n",
      "Requirement already satisfied: pillow-heif>=0.18.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.22.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: requests in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (4.57.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests->roboflow) (3.4.1)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.10.0\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall opencv-python-headless 4.10.0\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for opencv-python-headless.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: The package was installed by conda. You should check if it can uninstall the package.\n",
      "Requirement already satisfied: roboflow in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.1.61)\n",
      "Requirement already satisfied: certifi in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2025.1.31)\n",
      "Requirement already satisfied: idna==3.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.26.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (11.2.1)\n",
      "Requirement already satisfied: pillow-heif>=0.18.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.22.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: requests in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (4.57.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->roboflow) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from requests->roboflow) (3.4.1)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.10.0\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall opencv-python-headless 4.10.0\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for opencv-python-headless.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: The package was installed by conda. You should check if it can uninstall the package.\n",
      "Collecting roboflow\n",
      "  Using cached roboflow-1.1.61-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting certifi (from roboflow)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting cycler (from roboflow)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from roboflow)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting matplotlib (from roboflow)\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.18.5 (from roboflow)\n",
      "  Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting Pillow>=7.1.2 (from roboflow)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
      "  Using cached pillow_heif-0.22.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting python-dateutil (from roboflow)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting python-dotenv (from roboflow)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting requests (from roboflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting six (from roboflow)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting urllib3>=1.26.6 (from roboflow)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting tqdm>=4.41.0 (from roboflow)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML>=5.3.1 (from roboflow)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests-toolbelt (from roboflow)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filetype (from roboflow)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->roboflow)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->roboflow)\n",
      "  Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (102 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib->roboflow)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->roboflow)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->roboflow)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Using cached roboflow-1.1.61-py3-none-any.whl (85 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pillow_heif-0.22.0-cp312-cp312-macosx_14_0_arm64.whl (4.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: filetype, urllib3, tqdm, six, PyYAML, python-dotenv, pyparsing, Pillow, packaging, numpy, kiwisolver, idna, fonttools, cycler, charset-normalizer, certifi, requests, python-dateutil, pillow-heif, opencv-python-headless, contourpy, requests-toolbelt, matplotlib, roboflow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.5 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
      "conda 25.1.1 requires conda-libmamba-solver>=24.11.0, but you have conda-libmamba-solver 24.9.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pillow-11.2.1 PyYAML-6.0.1 certifi-2025.1.31 charset-normalizer-3.4.1 contourpy-1.3.2 cycler-0.11.0 filetype-1.2.0 fonttools-4.57.0 idna-3.7 kiwisolver-1.4.8 matplotlib-3.9.2 numpy-1.26.4 opencv-python-headless-4.10.0 packaging-24.1 pillow-heif-0.22.0 pyparsing-3.2.3 python-dateutil-2.9.0.post0 python-dotenv-0.21.0 requests-2.32.3 requests-toolbelt-1.0.0 roboflow-1.1.61 six-1.16.0 tqdm-4.66.5 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "!pip install roboflow --upgrade\n",
    "!pip install --ignore-installed roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "260110d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/mediapipe/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/mediapipe/tasks/python/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/mediapipe/tasks/python/audio/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n\u001b[1;32m     22\u001b[0m AudioClassifierOptions \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierOptions\n\u001b[1;32m     23\u001b[0m AudioClassifierResult \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifierResult\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "from roboflow import Roboflow\n",
    "import json\n",
    "\n",
    "\n",
    "class SpanishDanceAnalyzer:\n",
    "    \"\"\"Main class to analyze Spanish dance performances using pose estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, use_roboflow=True, roboflow_api_key=None):\n",
    "        # MediaPipe setup\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Roboflow setup for custom Spanish dance position detection\n",
    "        self.use_roboflow = use_roboflow\n",
    "        if use_roboflow and roboflow_api_key:\n",
    "            try:\n",
    "                rf = Roboflow(api_key=roboflow_api_key)\n",
    "                # Update with your actual workspace and project name\n",
    "                self.project = rf.workspace(\"your-workspace\").project(\"spanish-dance-positions\")\n",
    "                self.model = self.project.version(1).model\n",
    "                print(\"Roboflow model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load Roboflow model: {e}\")\n",
    "                self.use_roboflow = False\n",
    "        \n",
    "        # Define key landmarks for Spanish dance analysis\n",
    "        self.landmarks = {\n",
    "            # Upper body landmarks (for arm positions)\n",
    "            'left_wrist': self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            'right_wrist': self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            'left_elbow': self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            'right_elbow': self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            'left_shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            'right_shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            \n",
    "            # Lower body landmarks (for footwork)\n",
    "            'left_hip': self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            'right_hip': self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            'left_knee': self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            'right_knee': self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            'left_ankle': self.mp_pose.PoseLandmark.LEFT_ANKLE.value,\n",
    "            'right_ankle': self.mp_pose.PoseLandmark.RIGHT_ANKLE.value,\n",
    "            'left_foot_index': self.mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value,\n",
    "            'right_foot_index': self.mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value,\n",
    "        }\n",
    "        \n",
    "        # Landmark groups for specific analysis\n",
    "        self.arm_landmarks = [\n",
    "            'left_wrist', 'right_wrist', 'left_elbow', \n",
    "            'right_elbow', 'left_shoulder', 'right_shoulder'\n",
    "        ]\n",
    "        \n",
    "        self.foot_landmarks = [\n",
    "            'left_hip', 'right_hip', 'left_knee', 'right_knee',\n",
    "            'left_ankle', 'right_ankle', 'left_foot_index', 'right_foot_index'\n",
    "        ]\n",
    "        \n",
    "        # Storage for reference poses and choreography sequence\n",
    "        self.reference_poses = {}\n",
    "        self.choreography_sequence = []\n",
    "        \n",
    "        # Data directory for saving/loading\n",
    "        self.data_dir = \"spanish_dance_data\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_poses_from_video(self, video_path, skip_frames=2):\n",
    "        \"\"\"Process video and extract pose landmarks for each frame\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video: {video_path}\")\n",
    "            return [], [], None\n",
    "            \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        \n",
    "        print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds, Total frames: {frame_count}\")\n",
    "        \n",
    "        poses = []\n",
    "        frames = []\n",
    "        keyframes = []  # Store key frames for choreography points\n",
    "        processed_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            processed_count += 1\n",
    "            if processed_count % skip_frames != 0:\n",
    "                continue\n",
    "                \n",
    "            # Convert to RGB for MediaPipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process with MediaPipe\n",
    "            results = self.pose.process(rgb_frame)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Store frame and pose data\n",
    "                frames.append(frame)\n",
    "                \n",
    "                # Extract key landmarks\n",
    "                landmarks_dict = {}\n",
    "                for name, idx in self.landmarks.items():\n",
    "                    lm = results.pose_landmarks.landmark[idx]\n",
    "                    landmarks_dict[name] = {\n",
    "                        'x': lm.x, 'y': lm.y, 'z': lm.z, 'visibility': lm.visibility\n",
    "                    }\n",
    "                \n",
    "                pose_data = {\n",
    "                    'frame_idx': processed_count,\n",
    "                    'timestamp': processed_count / fps if fps > 0 else 0,\n",
    "                    'landmarks': landmarks_dict,\n",
    "                    'full_pose': results.pose_landmarks\n",
    "                }\n",
    "                \n",
    "                poses.append(pose_data)\n",
    "                \n",
    "                # Every 30 frames (adjusted by skip_frames), save a keyframe for reference\n",
    "                if processed_count % (30 * skip_frames) == 0:\n",
    "                    # Draw landmarks on frame for visualization\n",
    "                    annotated_frame = frame.copy()\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        annotated_frame, \n",
    "                        results.pose_landmarks,\n",
    "                        self.mp_pose.POSE_CONNECTIONS\n",
    "                    )\n",
    "                    keyframes.append({\n",
    "                        'frame_idx': processed_count,\n",
    "                        'timestamp': processed_count / fps if fps > 0 else 0,\n",
    "                        'frame': annotated_frame\n",
    "                    })\n",
    "            \n",
    "            # Show progress\n",
    "            if processed_count % 100 == 0:\n",
    "                print(f\"Processed {processed_count} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"Video processing complete. Extracted {len(poses)} poses.\")\n",
    "        return poses, frames, keyframes\n",
    "    \n",
    "    def normalize_pose_data(self, landmarks_dict):\n",
    "        \"\"\"Normalize pose data to make it scale and position invariant\"\"\"\n",
    "        if not landmarks_dict:\n",
    "            return None\n",
    "            \n",
    "        # Extract hip center coordinates\n",
    "        left_hip = np.array([\n",
    "            landmarks_dict['left_hip']['x'],\n",
    "            landmarks_dict['left_hip']['y'],\n",
    "            landmarks_dict['left_hip']['z']\n",
    "        ])\n",
    "        \n",
    "        right_hip = np.array([\n",
    "            landmarks_dict['right_hip']['x'],\n",
    "            landmarks_dict['right_hip']['y'],\n",
    "            landmarks_dict['right_hip']['z']\n",
    "        ])\n",
    "        \n",
    "        hip_center = (left_hip + right_hip) / 2\n",
    "        \n",
    "        # Calculate torso height for normalization\n",
    "        left_shoulder = np.array([\n",
    "            landmarks_dict['left_shoulder']['x'],\n",
    "            landmarks_dict['left_shoulder']['y'],\n",
    "            landmarks_dict['left_shoulder']['z']\n",
    "        ])\n",
    "        \n",
    "        right_shoulder = np.array([\n",
    "            landmarks_dict['right_shoulder']['x'],\n",
    "            landmarks_dict['right_shoulder']['y'],\n",
    "            landmarks_dict['right_shoulder']['z']\n",
    "        ])\n",
    "        \n",
    "        shoulder_center = (left_shoulder + right_shoulder) / 2\n",
    "        torso_height = np.linalg.norm(shoulder_center - hip_center)\n",
    "        \n",
    "        # Normalize landmarks relative to hip center and torso height\n",
    "        normalized_landmarks = {}\n",
    "        for name, lm in landmarks_dict.items():\n",
    "            point = np.array([lm['x'], lm['y'], lm['z']])\n",
    "            # Translate to make hip center the origin\n",
    "            normalized = point - hip_center\n",
    "            # Scale by torso height\n",
    "            if torso_height > 0:\n",
    "                normalized = normalized / torso_height\n",
    "            \n",
    "            normalized_landmarks[name] = {\n",
    "                'x': normalized[0],\n",
    "                'y': normalized[1],\n",
    "                'z': normalized[2],\n",
    "                'visibility': lm['visibility']\n",
    "            }\n",
    "        \n",
    "        return normalized_landmarks\n",
    "    \n",
    "    def normalize_all_poses(self, poses):\n",
    "        \"\"\"Normalize an array of poses\"\"\"\n",
    "        normalized_poses = []\n",
    "        for pose in poses:\n",
    "            norm_landmarks = self.normalize_pose_data(pose['landmarks'])\n",
    "            if norm_landmarks:\n",
    "                normalized_pose = pose.copy()\n",
    "                normalized_pose['normalized_landmarks'] = norm_landmarks\n",
    "                normalized_poses.append(normalized_pose)\n",
    "        \n",
    "        return normalized_poses\n",
    "    \n",
    "    def set_reference_demo(self, video_path):\n",
    "        \"\"\"Set the demonstration video as reference for analysis\"\"\"\n",
    "        poses, frames, keyframes = self.extract_poses_from_video(video_path)\n",
    "        self.reference_poses = self.normalize_all_poses(poses)\n",
    "        self.reference_keyframes = keyframes\n",
    "        \n",
    "        # Save reference data\n",
    "        self.save_reference_data()\n",
    "        \n",
    "        return len(self.reference_poses)\n",
    "    \n",
    "    def save_reference_data(self):\n",
    "        \"\"\"Save reference pose data to disk\"\"\"\n",
    "        # We don't save full frames to save disk space\n",
    "        serializable_poses = []\n",
    "        for pose in self.reference_poses:\n",
    "            pose_copy = pose.copy()\n",
    "            # Remove non-serializable objects\n",
    "            if 'full_pose' in pose_copy:\n",
    "                del pose_copy['full_pose']\n",
    "            serializable_poses.append(pose_copy)\n",
    "        \n",
    "        with open(os.path.join(self.data_dir, \"reference_poses.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(serializable_poses, f)\n",
    "        \n",
    "        # Save keyframes as images\n",
    "        keyframes_dir = os.path.join(self.data_dir, \"keyframes\")\n",
    "        os.makedirs(keyframes_dir, exist_ok=True)\n",
    "        \n",
    "        keyframe_data = []\n",
    "        for i, kf in enumerate(self.reference_keyframes):\n",
    "            filename = f\"keyframe_{i:03d}.jpg\"\n",
    "            filepath = os.path.join(keyframes_dir, filename)\n",
    "            cv2.imwrite(filepath, kf['frame'])\n",
    "            \n",
    "            keyframe_data.append({\n",
    "                'frame_idx': kf['frame_idx'],\n",
    "                'timestamp': kf['timestamp'],\n",
    "                'filename': filename\n",
    "            })\n",
    "        \n",
    "        with open(os.path.join(self.data_dir, \"keyframes.json\"), \"w\") as f:\n",
    "            json.dump(keyframe_data, f)\n",
    "    \n",
    "    def load_reference_data(self):\n",
    "        \"\"\"Load reference pose data from disk\"\"\"\n",
    "        ref_file = os.path.join(self.data_dir, \"reference_poses.pkl\")\n",
    "        if os.path.exists(ref_file):\n",
    "            with open(ref_file, \"rb\") as f:\n",
    "                self.reference_poses = pickle.load(f)\n",
    "            \n",
    "            # Load keyframe data\n",
    "            keyframes_data_file = os.path.join(self.data_dir, \"keyframes.json\")\n",
    "            keyframes_dir = os.path.join(self.data_dir, \"keyframes\")\n",
    "            \n",
    "            if os.path.exists(keyframes_data_file):\n",
    "                with open(keyframes_data_file, \"r\") as f:\n",
    "                    keyframe_data = json.load(f)\n",
    "                \n",
    "                self.reference_keyframes = []\n",
    "                for kf in keyframe_data:\n",
    "                    filepath = os.path.join(keyframes_dir, kf['filename'])\n",
    "                    if os.path.exists(filepath):\n",
    "                        frame = cv2.imread(filepath)\n",
    "                        self.reference_keyframes.append({\n",
    "                            'frame_idx': kf['frame_idx'],\n",
    "                            'timestamp': kf['timestamp'],\n",
    "                            'frame': frame\n",
    "                        })\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def define_choreography_sequence(self, timestamps, labels):\n",
    "        \"\"\"Define key moments in the choreography with labels\"\"\"\n",
    "        self.choreography_sequence = []\n",
    "        \n",
    "        for timestamp, label in zip(timestamps, labels):\n",
    "            # Find the closest pose to the timestamp\n",
    "            closest_pose = None\n",
    "            min_diff = float('inf')\n",
    "            \n",
    "            for pose in self.reference_poses:\n",
    "                diff = abs(pose['timestamp'] - timestamp)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    closest_pose = pose\n",
    "            \n",
    "            if closest_pose:\n",
    "                self.choreography_sequence.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'label': label,\n",
    "                    'pose_idx': closest_pose['frame_idx']\n",
    "                })\n",
    "        \n",
    "        # Save choreography sequence\n",
    "        with open(os.path.join(self.data_dir, \"choreography.json\"), \"w\") as f:\n",
    "            json.dump(self.choreography_sequence, f)\n",
    "        \n",
    "        return len(self.choreography_sequence)\n",
    "    \n",
    "    def load_choreography_sequence(self):\n",
    "        \"\"\"Load choreography sequence from disk\"\"\"\n",
    "        choreo_file = os.path.join(self.data_dir, \"choreography.json\")\n",
    "        if os.path.exists(choreo_file):\n",
    "            with open(choreo_file, \"r\") as f:\n",
    "                self.choreography_sequence = json.load(f)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def extract_features(self, landmarks_dict, landmark_group=\"all\"):\n",
    "        \"\"\"Extract pose features for comparison\"\"\"\n",
    "        if landmark_group == \"arms\":\n",
    "            target_landmarks = self.arm_landmarks\n",
    "        elif landmark_group == \"feet\":\n",
    "            target_landmarks = self.foot_landmarks\n",
    "        else:\n",
    "            target_landmarks = list(self.landmarks.keys())\n",
    "        \n",
    "        features = []\n",
    "        for name in target_landmarks:\n",
    "            if name in landmarks_dict:\n",
    "                lm = landmarks_dict[name]\n",
    "                features.extend([lm['x'], lm['y'], lm['z']])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def compute_pose_similarity(self, pose1, pose2, landmark_group=\"all\"):\n",
    "        \"\"\"Compute similarity between two poses\"\"\"\n",
    "        # Extract features from normalized landmarks\n",
    "        features1 = self.extract_features(pose1['normalized_landmarks'], landmark_group)\n",
    "        features2 = self.extract_features(pose2['normalized_landmarks'], landmark_group)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(features1.reshape(1, -1), features2.reshape(1, -1))[0][0]\n",
    "        \n",
    "        # Normalize to percentage (0-100%)\n",
    "        similarity_percent = (similarity + 1) * 50\n",
    "        \n",
    "        return similarity_percent\n",
    "    \n",
    "    def find_matching_sequence(self, student_poses, window_size=3):\n",
    "        \"\"\"Find the best matching sequence in student video for each choreography point\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for choreo_point in self.choreography_sequence:\n",
    "            # Find reference pose for this choreography point\n",
    "            ref_pose_idx = None\n",
    "            for i, pose in enumerate(self.reference_poses):\n",
    "                if pose['frame_idx'] == choreo_point['pose_idx']:\n",
    "                    ref_pose_idx = i\n",
    "                    break\n",
    "            \n",
    "            if ref_pose_idx is None:\n",
    "                continue\n",
    "            \n",
    "            ref_pose = self.reference_poses[ref_pose_idx]\n",
    "            \n",
    "            # Find best matching pose in student video\n",
    "            best_match_idx = -1\n",
    "            best_similarity = 0\n",
    "            \n",
    "            for i, student_pose in enumerate(student_poses):\n",
    "                # Check if we have enough window space\n",
    "                if i < len(student_poses) - window_size:\n",
    "                    # Compute average similarity over a window of poses\n",
    "                    window_similarity = 0\n",
    "                    for w in range(window_size):\n",
    "                        pose_similarity = self.compute_pose_similarity(\n",
    "                            ref_pose, student_poses[i + w]\n",
    "                        )\n",
    "                        window_similarity += pose_similarity\n",
    "                    \n",
    "                    window_similarity /= window_size\n",
    "                    \n",
    "                    if window_similarity > best_similarity:\n",
    "                        best_similarity = window_similarity\n",
    "                        best_match_idx = i\n",
    "            \n",
    "            # Determine arm and footwork specific similarity\n",
    "            arm_similarity = 0\n",
    "            foot_similarity = 0\n",
    "            \n",
    "            if best_match_idx >= 0:\n",
    "                best_student_pose = student_poses[best_match_idx]\n",
    "                arm_similarity = self.compute_pose_similarity(ref_pose, best_student_pose, \"arms\")\n",
    "                foot_similarity = self.compute_pose_similarity(ref_pose, best_student_pose, \"feet\")\n",
    "            \n",
    "            results.append({\n",
    "                'choreography_point': choreo_point,\n",
    "                'label': choreo_point['label'],\n",
    "                'reference_pose_idx': ref_pose_idx,\n",
    "                'student_pose_idx': best_match_idx,\n",
    "                'overall_similarity': best_similarity,\n",
    "                'arm_similarity': arm_similarity,\n",
    "                'foot_similarity': foot_similarity\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_student_performance(self, student_video_path):\n",
    "        \"\"\"Analyze a student's dance performance compared to reference\"\"\"\n",
    "        # Make sure we have reference data\n",
    "        if not self.reference_poses:\n",
    "            if not self.load_reference_data():\n",
    "                print(\"Error: No reference data available. Please set a reference demo first.\")\n",
    "                return None\n",
    "        \n",
    "        if not self.choreography_sequence:\n",
    "            if not self.load_choreography_sequence():\n",
    "                print(\"Error: No choreography sequence defined. Please define choreography first.\")\n",
    "                return None\n",
    "        \n",
    "        # Process student video\n",
    "        student_poses, _, student_keyframes = self.extract_poses_from_video(student_video_path)\n",
    "        normalized_student_poses = self.normalize_all_poses(student_poses)\n",
    "        \n",
    "        # Find matching sequences\n",
    "        matching_results = self.find_matching_sequence(normalized_student_poses)\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        overall_score = 0\n",
    "        arm_score = 0\n",
    "        foot_score = 0\n",
    "        \n",
    "        if matching_results:\n",
    "            overall_score = sum(r['overall_similarity'] for r in matching_results) / len(matching_results)\n",
    "            arm_score = sum(r['arm_similarity'] for r in matching_results) / len(matching_results)\n",
    "            foot_score = sum(r['foot_similarity'] for r in matching_results) / len(matching_results)\n",
    "        \n",
    "        performance_analysis = {\n",
    "            'student_video': student_video_path,\n",
    "            'poses_extracted': len(student_poses),\n",
    "            'matching_results': matching_results,\n",
    "            'overall_score': overall_score,\n",
    "            'arm_score': arm_score,\n",
    "            'foot_score': foot_score\n",
    "        }\n",
    "        \n",
    "        return performance_analysis\n",
    "    \n",
    "    def generate_feedback(self, performance_analysis):\n",
    "        \"\"\"Generate detailed feedback from performance analysis\"\"\"\n",
    "        if not performance_analysis:\n",
    "            return \"No performance analysis data available.\"\n",
    "        \n",
    "        overall_score = performance_analysis['overall_score']\n",
    "        arm_score = performance_analysis['arm_score']\n",
    "        foot_score = performance_analysis['foot_score']\n",
    "        \n",
    "        results = performance_analysis['matching_results']\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html = f\"\"\"\n",
    "        <h2>Spanish Dance Performance Evaluation</h2>\n",
    "        <h3>Overall Technique Mark: {overall_score:.1f}%</h3>\n",
    "        <div style=\"margin-bottom: 20px;\">\n",
    "            <div style=\"display: inline-block; margin-right: 20px;\">\n",
    "                <strong>Arm Movements:</strong> {arm_score:.1f}%\n",
    "            </div>\n",
    "            <div style=\"display: inline-block;\">\n",
    "                <strong>Footwork:</strong> {foot_score:.1f}%\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h3>Choreography Evaluation</h3>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
    "            <tr style=\"background-color: #f2f2f2;\">\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Position/Step</th>\n",
    "                <th style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">Execution</th>\n",
    "                <th style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">Similarity</th>\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Feedback</th>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add rows for each choreography point\n",
    "        for i, result in enumerate(results):\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            arm_sim = result['arm_similarity']\n",
    "            foot_sim = result['foot_similarity']\n",
    "            \n",
    "            # Determine execution rating\n",
    "            if similarity >= 90:\n",
    "                execution = \"Excellent\"\n",
    "                color = \"green\"\n",
    "                feedback = f\"Excellent execution of {label}. Perfect arm position ({arm_sim:.1f}%) and footwork ({foot_sim:.1f}%).\"\n",
    "            elif similarity >= 75:\n",
    "                execution = \"Good\"\n",
    "                color = \"#5cb85c\"  # lighter green\n",
    "                feedback = f\"Good execution of {label}. \"\n",
    "                if arm_sim < 75:\n",
    "                    feedback += f\"Focus on improving arm positions ({arm_sim:.1f}%). \"\n",
    "                if foot_sim < 75:\n",
    "                    feedback += f\"Pay attention to footwork details ({foot_sim:.1f}%). \"\n",
    "            elif similarity >= 60:\n",
    "                execution = \"Fair\"\n",
    "                color = \"orange\"\n",
    "                feedback = f\"Fair execution of {label}. \"\n",
    "                if arm_sim < foot_sim:\n",
    "                    feedback += f\"Arm positions need more practice ({arm_sim:.1f}%). \"\n",
    "                else:\n",
    "                    feedback += f\"Footwork needs refinement ({foot_sim:.1f}%). \"\n",
    "            else:\n",
    "                execution = \"Needs Improvement\"\n",
    "                color = \"red\"\n",
    "                feedback = f\"The {label} position needs significant improvement. Review the demonstration video carefully.\"\n",
    "            \n",
    "            # Add table row\n",
    "            html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"padding: 8px; border: 1px solid #ddd;\">{label}</td>\n",
    "                <td style=\"padding: 8px; text-align: center; color: {color}; border: 1px solid #ddd;\">{execution}</td>\n",
    "                <td style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">{similarity:.1f}%</td>\n",
    "                <td style=\"padding: 8px; border: 1px solid #ddd;\">{feedback}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Close table and add summary\n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Summary Feedback</h3>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate overall feedback based on scores\n",
    "        if overall_score >= 85:\n",
    "            html += \"\"\"\n",
    "            <p>Outstanding performance! Your technique closely matches the demonstration video. \n",
    "            Continue refining the minor details for even greater precision in your Spanish dance execution.</p>\n",
    "            \"\"\"\n",
    "        elif overall_score >= 70:\n",
    "            html += \"\"\"\n",
    "            <p>Good performance with strong technical foundations. Focus on the positions where your similarity\n",
    "            score was lower, and pay attention to the precise arm positions and footwork timing.</p>\n",
    "            \"\"\"\n",
    "        elif overall_score >= 50:\n",
    "            html += \"\"\"\n",
    "            <p>Fair performance with room for improvement. Practice the choreography sequence regularly,\n",
    "            focusing particularly on the positions marked \"Needs Improvement\" or \"Fair\".</p>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            html += \"\"\"\n",
    "            <p>This choreography needs more practice. Start by focusing on each position individually\n",
    "            before attempting the full sequence. Use the demonstration video as a reference and\n",
    "            practice in front of a mirror.</p>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add areas for improvement\n",
    "        html += \"<h3>Priority Areas for Improvement</h3><ul>\"\n",
    "        \n",
    "        # Sort results by similarity (ascending) to find weakest areas\n",
    "        sorted_results = sorted(results, key=lambda x: x['overall_similarity'])\n",
    "        for result in sorted_results[:3]:  # Top 3 weakest areas\n",
    "            html += f\"<li><strong>{result['label']}</strong> ({result['overall_similarity']:.1f}%)</li>\"\n",
    "        \n",
    "        html += \"</ul>\"\n",
    "        \n",
    "        # Add encouragement\n",
    "        html += \"\"\"\n",
    "        <p style=\"margin-top: 20px;\">\n",
    "            Remember that consistent practice is key to mastering Spanish dance techniques.\n",
    "            Focus on one improvement area at a time for the best results.\n",
    "        </p>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def visualize_comparison(self, performance_analysis, output_path=None):\n",
    "        \"\"\"Create a visual comparison of reference and student poses\"\"\"\n",
    "        if not performance_analysis or not performance_analysis['matching_results']:\n",
    "            return None\n",
    "        \n",
    "        results = performance_analysis['matching_results']\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        rows = len(results)\n",
    "        fig, axs = plt.subplots(rows, 2, figsize=(12, 4 * rows))\n",
    "        \n",
    "        if rows == 1:\n",
    "            axs = [axs]  # Make it iterable for single row\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            \n",
    "            # Find reference pose\n",
    "            ref_pose_idx = result['reference_pose_idx']\n",
    "            ref_pose = self.reference_poses[ref_pose_idx]\n",
    "            \n",
    "            # Find the nearest keyframe\n",
    "            ref_keyframe = None\n",
    "            min_dist = float('inf')\n",
    "            for kf in self.reference_keyframes:\n",
    "                dist = abs(kf['frame_idx'] - ref_pose['frame_idx'])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    ref_keyframe = kf\n",
    "            \n",
    "            if ref_keyframe:\n",
    "                ref_frame = ref_keyframe['frame']\n",
    "                \n",
    "                # Find student pose\n",
    "                student_pose_idx = result['student_pose_idx']\n",
    "                if student_pose_idx >= 0:\n",
    "                    # We would need to have saved student frames, which we don't for memory reasons\n",
    "                    # Placeholder for actual student frame visualization\n",
    "                    student_frame = np.zeros_like(ref_frame)\n",
    "                    \n",
    "                    # Draw pose comparison\n",
    "                    axs[i][0].imshow(cv2.cvtColor(ref_frame, cv2.COLOR_BGR2RGB))\n",
    "                    axs[i][0].set_title(f\"Reference: {label}\")\n",
    "                    axs[i][0].axis('off')\n",
    "                    \n",
    "                    axs[i][1].imshow(cv2.cvtColor(student_frame, cv2.COLOR_BGR2RGB))\n",
    "                    axs[i][1].set_title(f\"Student: {similarity:.1f}% similarity\")\n",
    "                    axs[i][1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            return output_path\n",
    "        else:\n",
    "            plt.show()\n",
    "            return fig\n",
    "\n",
    "\n",
    "# GUI Application for the Spanish Dance Analysis Tool\n",
    "class SpanishDanceApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Spanish Dance Analysis Tool\")\n",
    "        self.root.geometry(\"1000x700\")\n",
    "        \n",
    "        # Initialize the analyzer\n",
    "        self.analyzer = SpanishDanceAnalyzer()\n",
    "        \n",
    "        # Try to load existing data\n",
    "        self.analyzer.load_reference_data()\n",
    "        self.analyzer.load_choreography_sequence()\n",
    "        \n",
    "        # Create the main interface\n",
    "        self.create_interface()\n",
    "        \n",
    "        # Status variables\n",
    "        self.demo_video_path = None\n",
    "        self.student_video_path = None\n",
    "        self.performance_results = None\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the application interface\"\"\"\n",
    "        # Create a notebook with tabs\n",
    "        notebook = ttk.Notebook(self.root)\n",
    "        notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Tab 1: Setup and Configuration\n",
    "        setup_tab = ttk.Frame(notebook)\n",
    "        notebook.add(setup_tab, text=\"Setup\")\n",
    "        self.create_setup_tab(setup_tab)\n",
    "        \n",
    "        # Tab 2: Student Evaluation\n",
    "        evaluation_tab = ttk.Frame(notebook)\n",
    "        notebook.add(evaluation_tab, text=\"Evaluation\")\n",
    "        self.create_evaluation_tab(evaluation_tab)\n",
    "        \n",
    "        # Tab 3: Results and Reports\n",
    "        results_tab = ttk.Frame(notebook)\n",
    "        notebook.add(results_tab, text=\"Results\")\n",
    "        self.create_results_tab(results_tab)\n",
    "        \n",
    "        # Status bar\n",
    "        status_frame = ttk.Frame(self.root)\n",
    "        status_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        status_label = ttk.Label(status_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_label.pack(fill=tk.X)\n",
    "    \n",
    "    def create_setup_tab(self, parent):\n",
    "        \"\"\"Create the setup tab interface\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Demo Video Section\n",
    "        demo_frame = ttk.LabelFrame(frame, text=\"Step 1: Set Demonstration Video\", padding=10)\n",
    "        demo_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(demo_frame, text=\"Select a video showing the correct dance choreography:\").pack(anchor=tk.W)\n",
    "        \n",
    "        btn_frame = ttk.Frame(demo_frame)\n",
    "        btn_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(btn_frame, text=\"Load Demo Video\", command=self.load_demo_video).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.demo_status_var = tk.StringVar()\n",
    "        self.demo_status_var.set(\"No demo video loaded\")\n",
    "        ttk.Label(demo_frame, textvariable=self.demo_status_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # 2. Choreography Definition Section\n",
    "        choreo_frame = ttk.LabelFrame(frame, text=\"Step 2: Define Choreography Sequence\", padding=10)\n",
    "        choreo_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(choreo_frame, text=\"After loading the demo video, define key dance positions/steps:\").pack(anchor=tk.W)\n",
    "        \n",
    "        choreo_controls = ttk.Frame(choreo_frame)\n",
    "        choreo_controls.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.choreo_list = ttk.Treeview(choreo_frame, columns=(\"Timestamp\", \"Label\"), show=\"headings\", height=6)\n",
    "        self.choreo_list.heading(\"Timestamp\", text=\"Time (seconds)\")\n",
    "        self.choreo_list.heading(\"Label\", text=\"Position/Step Name\")\n",
    "        self.choreo_list.column(\"Timestamp\", width=100)\n",
    "        self.choreo_list.column(\"Label\", width=300)\n",
    "        self.choreo_list.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        choreo_buttons = ttk.Frame(choreo_frame)\n",
    "        choreo_buttons.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(choreo_buttons, text=\"Add Position\", command=self.add_choreography_point).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(choreo_buttons, text=\"Remove Selected\", command=self.remove_choreography_point).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(choreo_buttons, text=\"Save Sequence\", command=self.save_choreography).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # 3. Configuration Section\n",
    "        config_frame = ttk.LabelFrame(frame, text=\"Step 3: Configuration\", padding=10)\n",
    "        config_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(config_frame, text=\"Add your Roboflow API key (optional):\").pack(anchor=tk.W)\n",
    "        \n",
    "        api_frame = ttk.Frame(config_frame)\n",
    "        api_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.api_key_var = tk.StringVar()\n",
    "        api_entry = ttk.Entry(api_frame, textvariable=self.api_key_var, width=40)\n",
    "        api_entry.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        ttk.Button(api_frame, text=\"Set API Key\", command=self.set_api_key).pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    def create_evaluation_tab(self, parent):\n",
    "        \"\"\"Create the evaluation tab interface\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Student Video Section\n",
    "        student_frame = ttk.LabelFrame(frame, text=\"Step 1: Load Student Performance\", padding=10)\n",
    "        student_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(student_frame, text=\"Select a video of the student performing the choreography:\").pack(anchor=tk.W)\n",
    "        \n",
    "        btn_frame = ttk.Frame(student_frame)\n",
    "        btn_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(btn_frame, text=\"Load Student Video\", command=self.load_student_video).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.student_status_var = tk.StringVar()\n",
    "        self.student_status_var.set(\"No student video loaded\")\n",
    "        ttk.Label(student_frame, textvariable=self.student_status_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # 2. Analysis Section\n",
    "        analysis_frame = ttk.LabelFrame(frame, text=\"Step 2: Analyze Performance\", padding=10)\n",
    "        analysis_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(analysis_frame, text=\"Compare the student's performance with the demonstration:\").pack(anchor=tk.W)\n",
    "        \n",
    "        ttk.Button(analysis_frame, text=\"Analyze Performance\", command=self.analyze_performance).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        self.analysis_progress = ttk.Progressbar(analysis_frame, orient=tk.HORIZONTAL, mode='indeterminate')\n",
    "        self.analysis_progress.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.analysis_status_var = tk.StringVar()\n",
    "        self.analysis_status_var.set(\"Ready for analysis\")\n",
    "        ttk.Label(analysis_frame, textvariable=self.analysis_status_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # 3. Preview Section\n",
    "        preview_frame = ttk.LabelFrame(frame, text=\"Preview\", padding=10)\n",
    "        preview_frame.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        preview_container = ttk.Frame(preview_frame)\n",
    "        preview_container.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.demo_preview = ttk.LabelFrame(preview_container, text=\"Demonstration\")\n",
    "        self.demo_preview.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.demo_canvas = tk.Canvas(self.demo_preview, bg='black')\n",
    "        self.demo_canvas.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.student_preview = ttk.LabelFrame(preview_container, text=\"Student Performance\")\n",
    "        self.student_preview.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.student_canvas = tk.Canvas(self.student_preview, bg='black')\n",
    "        self.student_canvas.pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    def create_results_tab(self, parent):\n",
    "        \"\"\"Create the results tab interface\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Summary Section\n",
    "        summary_frame = ttk.LabelFrame(frame, text=\"Performance Summary\", padding=10)\n",
    "        summary_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        summary_grid = ttk.Frame(summary_frame)\n",
    "        summary_grid.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Overall Technique Mark:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.overall_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.overall_score_var, font=('TkDefaultFont', 12, 'bold')).grid(row=0, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Arm Movements:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.arm_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.arm_score_var).grid(row=1, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Footwork:\").grid(row=2, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.foot_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.foot_score_var).grid(row=2, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        # 2. Detailed Results Section\n",
    "        results_frame = ttk.LabelFrame(frame, text=\"Detailed Results\", padding=10)\n",
    "        results_frame.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        # Treeview for results table\n",
    "        self.results_tree = ttk.Treeview(results_frame, columns=(\"Position\", \"Execution\", \"Similarity\", \"Feedback\"), show=\"headings\", height=10)\n",
    "        self.results_tree.heading(\"Position\", text=\"Position/Step\")\n",
    "        self.results_tree.heading(\"Execution\", text=\"Execution\")\n",
    "        self.results_tree.heading(\"Similarity\", text=\"Similarity\")\n",
    "        self.results_tree.heading(\"Feedback\", text=\"Feedback\")\n",
    "        \n",
    "        self.results_tree.column(\"Position\", width=150)\n",
    "        self.results_tree.column(\"Execution\", width=100)\n",
    "        self.results_tree.column(\"Similarity\", width=80)\n",
    "        self.results_tree.column(\"Feedback\", width=400)\n",
    "        \n",
    "        self.results_tree.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        # 3. Export Section\n",
    "        export_frame = ttk.Frame(frame)\n",
    "        export_frame.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "        ttk.Button(export_frame, text=\"Generate Detailed Report\", command=self.generate_report).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(export_frame, text=\"Export Results\", command=self.export_results).pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    def load_demo_video(self):\n",
    "        \"\"\"Load and process demonstration video\"\"\"\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            title=\"Select Demonstration Video\",\n",
    "            filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        self.demo_video_path = filepath\n",
    "        self.demo_status_var.set(f\"Processing: {os.path.basename(filepath)}...\")\n",
    "        self.status_var.set(\"Processing demonstration video. This may take a while...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Process in a separate thread to not freeze the UI\n",
    "        def process_video():\n",
    "            count = self.analyzer.set_reference_demo(filepath)\n",
    "            \n",
    "            # Update UI in the main thread\n",
    "            self.root.after(0, lambda: self.demo_status_var.set(f\"Loaded: {os.path.basename(filepath)} - {count} poses extracted\"))\n",
    "            self.root.after(0, lambda: self.status_var.set(\"Demonstration video processed successfully\"))\n",
    "        \n",
    "        threading.Thread(target=process_video).start()\n",
    "    \n",
    "    def add_choreography_point(self):\n",
    "        \"\"\"Add a choreography point to the sequence\"\"\"\n",
    "        if not self.demo_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a demonstration video first\")\n",
    "            return\n",
    "        \n",
    "        # Simple dialog to get timestamp and label\n",
    "        dialog = tk.Toplevel(self.root)\n",
    "        dialog.title(\"Add Choreography Point\")\n",
    "        dialog.geometry(\"300x150\")\n",
    "        dialog.resizable(False, False)\n",
    "        \n",
    "        ttk.Label(dialog, text=\"Time (seconds):\").pack(anchor=tk.W, padx=10, pady=5)\n",
    "        time_var = tk.DoubleVar()\n",
    "        time_entry = ttk.Entry(dialog, textvariable=time_var)\n",
    "        time_entry.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Label(dialog, text=\"Position/Step Name:\").pack(anchor=tk.W, padx=10, pady=5)\n",
    "        label_var = tk.StringVar()\n",
    "        label_entry = ttk.Entry(dialog, textvariable=label_var)\n",
    "        label_entry.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        def add_and_close():\n",
    "            timestamp = time_var.get()\n",
    "            label = label_var.get()\n",
    "            \n",
    "            if timestamp <= 0 or not label:\n",
    "                messagebox.showwarning(\"Warning\", \"Please enter valid time and position name\")\n",
    "                return\n",
    "            \n",
    "            self.choreo_list.insert(\"\", \"end\", values=(f\"{timestamp:.2f}\", label))\n",
    "            dialog.destroy()\n",
    "        \n",
    "        ttk.Button(dialog, text=\"Add\", command=add_and_close).pack(anchor=tk.CENTER, pady=10)\n",
    "        \n",
    "        # Set dialog modal\n",
    "        dialog.transient(self.root)\n",
    "        dialog.grab_set()\n",
    "        self.root.wait_window(dialog)\n",
    "    \n",
    "    def remove_choreography_point(self):\n",
    "        \"\"\"Remove selected choreography point\"\"\"\n",
    "        selected = self.choreo_list.selection()\n",
    "        if selected:\n",
    "            self.choreo_list.delete(selected)\n",
    "    \n",
    "    def save_choreography(self):\n",
    "        \"\"\"Save the defined choreography sequence\"\"\"\n",
    "        items = self.choreo_list.get_children()\n",
    "        if not items:\n",
    "            messagebox.showwarning(\"Warning\", \"No choreography points defined\")\n",
    "            return\n",
    "        \n",
    "        # Extract timestamps and labels\n",
    "        timestamps = []\n",
    "        labels = []\n",
    "        \n",
    "        for item in items:\n",
    "            values = self.choreo_list.item(item, \"values\")\n",
    "            timestamps.append(float(values[0]))\n",
    "            labels.append(values[1])\n",
    "        \n",
    "        # Save to analyzer\n",
    "        count = self.analyzer.define_choreography_sequence(timestamps, labels)\n",
    "        \n",
    "        self.status_var.set(f\"Choreography sequence saved: {count} positions\")\n",
    "        messagebox.showinfo(\"Success\", f\"Choreography sequence with {count} positions saved successfully\")\n",
    "    \n",
    "    def set_api_key(self):\n",
    "        \"\"\"Set Roboflow API key\"\"\"\n",
    "        api_key = self.api_key_var.get().strip()\n",
    "        if not api_key:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a valid API key\")\n",
    "            return\n",
    "        \n",
    "        # Reinitialize analyzer with API key\n",
    "        self.analyzer = SpanishDanceAnalyzer(use_roboflow=True, roboflow_api_key=api_key)\n",
    "        \n",
    "        # Reload data\n",
    "        self.analyzer.load_reference_data()\n",
    "        self.analyzer.load_choreography_sequence()\n",
    "        \n",
    "        self.status_var.set(\"Roboflow API key set successfully\")\n",
    "        messagebox.showinfo(\"Success\", \"Roboflow API key set successfully\")\n",
    "    \n",
    "    def load_student_video(self):\n",
    "        \"\"\"Load student performance video\"\"\"\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            title=\"Select Student Performance Video\",\n",
    "            filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        self.student_video_path = filepath\n",
    "        self.student_status_var.set(f\"Selected: {os.path.basename(filepath)}\")\n",
    "        self.status_var.set(f\"Student video selected: {os.path.basename(filepath)}\")\n",
    "    \n",
    "    def analyze_performance(self):\n",
    "        \"\"\"Analyze student performance compared to demonstration\"\"\"\n",
    "        if not self.demo_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a demonstration video first\")\n",
    "            return\n",
    "        \n",
    "        if not self.student_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a student performance video\")\n",
    "            return\n",
    "        \n",
    "        # Start progress bar\n",
    "        self.analysis_progress.start()\n",
    "        self.analysis_status_var.set(\"Analyzing performance... Please wait.\")\n",
    "        self.status_var.set(\"Analyzing student performance. This may take a while...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Process in a separate thread\n",
    "        def run_analysis():\n",
    "            # Analyze performance\n",
    "            results = self.analyzer.analyze_student_performance(self.student_video_path)\n",
    "            \n",
    "            # Update UI in main thread\n",
    "            self.root.after(0, lambda: self.update_results(results))\n",
    "        \n",
    "        threading.Thread(target=run_analysis).start()\n",
    "    \n",
    "    def update_results(self, results):\n",
    "        \"\"\"Update the UI with analysis results\"\"\"\n",
    "        # Stop progress bar\n",
    "        self.analysis_progress.stop()\n",
    "        \n",
    "        if not results:\n",
    "            self.analysis_status_var.set(\"Analysis failed. Please check logs.\")\n",
    "            self.status_var.set(\"Performance analysis failed\")\n",
    "            return\n",
    "        \n",
    "        self.performance_results = results\n",
    "        \n",
    "        # Update status\n",
    "        self.analysis_status_var.set(\"Analysis complete\")\n",
    "        self.status_var.set(\"Performance analysis completed successfully\")\n",
    "        \n",
    "        # Update scores in results tab\n",
    "        self.overall_score_var.set(f\"{results['overall_score']:.1f}%\")\n",
    "        self.arm_score_var.set(f\"{results['arm_score']:.1f}%\")\n",
    "        self.foot_score_var.set(f\"{results['foot_score']:.1f}%\")\n",
    "        \n",
    "        # Clear previous results\n",
    "        for i in self.results_tree.get_children():\n",
    "            self.results_tree.delete(i)\n",
    "        \n",
    "        # Add results to treeview\n",
    "        for result in results['matching_results']:\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            \n",
    "            # Determine execution rating\n",
    "            if similarity >= 90:\n",
    "                execution = \"Excellent\"\n",
    "            elif similarity >= 75:\n",
    "                execution = \"Good\"\n",
    "            elif similarity >= 60:\n",
    "                execution = \"Fair\"\n",
    "            else:\n",
    "                execution = \"Needs Improvement\"\n",
    "            \n",
    "            # Generate simple feedback\n",
    "            if similarity >= 90:\n",
    "                feedback = f\"Excellent execution of {label}\"\n",
    "            elif similarity >= 75:\n",
    "                feedback = f\"Good execution of {label} with minor adjustments needed\"\n",
    "            elif similarity >= 60:\n",
    "                feedback = f\"Fair execution of {label}, focus on form and technique\"\n",
    "            else:\n",
    "                feedback = f\"{label} needs significant improvement\"\n",
    "            \n",
    "            self.results_tree.insert(\"\", \"end\", values=(label, execution, f\"{similarity:.1f}%\", feedback))\n",
    "        \n",
    "        # Show comparison visualization\n",
    "        # This would typically display the comparison frames, but we're keeping it simple here\n",
    "        messagebox.showinfo(\"Analysis Complete\", f\"Performance analysis complete. Overall score: {results['overall_score']:.1f}%\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate and display detailed performance report\"\"\"\n",
    "        if not self.performance_results:\n",
    "            messagebox.showwarning(\"Warning\", \"No analysis results available\")\n",
    "            return\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html_report = self.analyzer.generate_feedback(self.performance_results)\n",
    "        \n",
    "        # Save HTML to temporary file\n",
    "        report_path = os.path.join(self.analyzer.data_dir, \"performance_report.html\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        # Open in default browser\n",
    "        import webbrowser\n",
    "        webbrowser.open(report_path)\n",
    "        \n",
    "        self.status_var.set(f\"Report generated and saved to {report_path}\")\n",
    "    \n",
    "    def export_results(self):\n",
    "        \"\"\"Export analysis results to a file\"\"\"\n",
    "        if not self.performance_results:\n",
    "            messagebox.showwarning(\"Warning\", \"No analysis results available\")\n",
    "            return\n",
    "        \n",
    "        filepath = filedialog.asksaveasfilename(\n",
    "            title=\"Save Results\",\n",
    "            defaultextension=\".json\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        # Create a serializable copy of results (removing non-JSON serializable objects)\n",
    "        results_copy = self.performance_results.copy()\n",
    "        \n",
    "        # Save to JSON\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results_copy, f, indent=4)\n",
    "        \n",
    "        self.status_var.set(f\"Results exported to {filepath}\")\n",
    "        messagebox.showinfo(\"Success\", f\"Results exported successfully to {filepath}\")\n",
    "\n",
    "\n",
    "# Main entry point\n",
    "def main():\n",
    "    \"\"\"Main entry point of the application\"\"\"\n",
    "    root = tk.Tk()\n",
    "    app = SpanishDanceApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64311302-0100-45f6-8838-d4a61053e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741270696.886746 2568791 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n"
     ]
    }
   ],
   "source": [
    "# MEDIAPIPE POSE ESTIMATION SETUP\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Initialize MediaPipe pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,  # Set to False for video\n",
    "    model_complexity=2,      # Higher accuracy\n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da59a9bf-10f7-4aaa-a759-a8f6be2d6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "def process_image(image_path):\n",
    "    \"\"\"Process an image and extract pose landmarks\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    return image, image_rgb, results\n",
    "\n",
    "def visualize_pose(image_rgb, results):\n",
    "    \"\"\"Visualize detected pose landmarks on an image\"\"\"\n",
    "    annotated_image = image_rgb.copy()\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS\n",
    "    )\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.title(\"Detected Pose\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b35c48-dbcf-4dc0-b2cc-ab8f90613fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BODY PART MAPPING\n",
    "\n",
    "# Define constants for pose comparison\n",
    "POSE_LANDMARKS = {\n",
    "    'left_wrist': mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "    'right_wrist': mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "    'left_elbow': mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "    'right_elbow': mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "    'left_shoulder': mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "    'right_shoulder': mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "    'left_hip': mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "    'right_hip': mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "    'left_knee': mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "    'right_knee': mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "    'left_ankle': mp_pose.PoseLandmark.LEFT_ANKLE.value,\n",
    "    'right_ankle': mp_pose.PoseLandmark.RIGHT_ANKLE.value,\n",
    "    'left_foot_index': mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value,\n",
    "    'right_foot_index': mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value,\n",
    "}\n",
    "\n",
    "# Define groups for specific analysis\n",
    "ARM_LANDMARKS = [\n",
    "    'left_wrist', 'right_wrist', 'left_elbow', 'right_elbow', \n",
    "    'left_shoulder', 'right_shoulder'\n",
    "]\n",
    "\n",
    "FOOT_LANDMARKS = [\n",
    "    'left_hip', 'right_hip', 'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle', 'left_foot_index', 'right_foot_index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f646aa57-07db-42f0-82bd-c5f17e802a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE POSE MANAGER CLASS\n",
    "class ReferencePoseManager:\n",
    "    \"\"\"Manages reference poses for Spanish dance movements\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"spanish_dance_references\"):\n",
    "        \"\"\"Initialize the reference pose manager\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.reference_poses = {}\n",
    "        self.ensure_data_dir()\n",
    "        \n",
    "    def ensure_data_dir(self):\n",
    "        \"\"\"Ensure the data directory exists\"\"\"\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "            \n",
    "    def add_reference_pose(self, pose_name, image_path, pose_type=\"arm\", description=\"\"):\n",
    "        \"\"\"Add a reference pose to the database\"\"\"\n",
    "        image, _, results = process_image(image_path)\n",
    "        \n",
    "        if not results.pose_landmarks:\n",
    "            print(f\"No pose detected in {image_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Extract landmarks as normalized vectors\n",
    "        landmarks = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.append([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        \n",
    "        self.reference_poses[pose_name] = {\n",
    "            'landmarks': landmarks,\n",
    "            'type': pose_type,  # 'arm' or 'foot'\n",
    "            'description': description,\n",
    "            'image_path': image_path\n",
    "        }\n",
    "        \n",
    "        # Save to disk\n",
    "        self.save_references()\n",
    "        return True\n",
    "    \n",
    "    def save_references(self):\n",
    "        \"\"\"Save reference poses to disk\"\"\"\n",
    "        with open(os.path.join(self.data_dir, \"reference_poses.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.reference_poses, f)\n",
    "    \n",
    "    def load_references(self):\n",
    "        \"\"\"Load reference poses from disk\"\"\"\n",
    "        ref_file = os.path.join(self.data_dir, \"reference_poses.pkl\")\n",
    "        if os.path.exists(ref_file):\n",
    "            with open(ref_file, \"rb\") as f:\n",
    "                self.reference_poses = pickle.load(f)\n",
    "    \n",
    "    def visualize_reference_pose(self, pose_name):\n",
    "        \"\"\"Visualize a reference pose\"\"\"\n",
    "        if pose_name not in self.reference_poses:\n",
    "            print(f\"Reference pose '{pose_name}' not found\")\n",
    "            return\n",
    "        \n",
    "        ref_data = self.reference_poses[pose_name]\n",
    "        image, image_rgb, results = process_image(ref_data['image_path'])\n",
    "        \n",
    "        print(f\"Reference Pose: {pose_name}\")\n",
    "        print(f\"Type: {ref_data['type']}\")\n",
    "        print(f\"Description: {ref_data['description']}\")\n",
    "        \n",
    "        visualize_pose(image_rgb, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281cee77-4a63-4fe0-974c-6b7fe0954a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSE ANALYZER CLASS\n",
    "class PoseAnalyzer:\n",
    "    \"\"\"Analyzes dancer poses against reference poses\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_manager):\n",
    "        \"\"\"Initialize the pose analyzer\"\"\"\n",
    "        self.ref_manager = reference_manager\n",
    "        \n",
    "    def normalize_pose(self, landmarks):\n",
    "        \"\"\"Normalize pose by centering and scaling\"\"\"\n",
    "        # Extract x, y coordinates\n",
    "        coords = np.array([[lm.x, lm.y] for lm in landmarks])\n",
    "        \n",
    "        # Find the bounding box\n",
    "        min_x, min_y = np.min(coords, axis=0)\n",
    "        max_x, max_y = np.max(coords, axis=0)\n",
    "        \n",
    "        # Scale and center\n",
    "        scale_x = 1.0 / (max_x - min_x) if max_x > min_x else 1.0\n",
    "        scale_y = 1.0 / (max_y - min_y) if max_y > min_y else 1.0\n",
    "        \n",
    "        normalized = []\n",
    "        for lm in landmarks:\n",
    "            normalized_lm = mp_pose.PoseLandmark()\n",
    "            normalized_lm.x = (lm.x - min_x) * scale_x\n",
    "            normalized_lm.y = (lm.y - min_y) * scale_y\n",
    "            normalized_lm.z = lm.z  # Keep z as is\n",
    "            normalized_lm.visibility = lm.visibility\n",
    "            normalized.append(normalized_lm)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def extract_landmark_vectors(self, landmarks, landmark_group):\n",
    "        \"\"\"Extract vectors for specific landmark groups (arm or foot)\"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        if landmark_group == \"arm\":\n",
    "            target_landmarks = ARM_LANDMARKS\n",
    "        elif landmark_group == \"foot\":\n",
    "            target_landmarks = FOOT_LANDMARKS\n",
    "        else:\n",
    "            # Use all landmarks\n",
    "            return np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten().reshape(1, -1)\n",
    "        \n",
    "        # Extract coordinates for target landmarks\n",
    "        coords = []\n",
    "        for name in target_landmarks:\n",
    "            idx = POSE_LANDMARKS[name]\n",
    "            lm = landmarks[idx]\n",
    "            coords.extend([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        return np.array(coords).reshape(1, -1)\n",
    "    \n",
    "    def compare_poses(self, dancer_landmarks, ref_pose_name, view=\"front\"):\n",
    "        \"\"\"Compare dancer pose to a reference pose\"\"\"\n",
    "        if ref_pose_name not in self.ref_manager.reference_poses:\n",
    "            return {\"score\": 0, \"message\": f\"Reference pose '{ref_pose_name}' not found\"}\n",
    "        \n",
    "        ref_data = self.ref_manager.reference_poses[ref_pose_name]\n",
    "        \n",
    "        # Determine which landmark group to use based on view and pose type\n",
    "        landmark_group = None\n",
    "        if view == \"front\" and ref_data['type'] == 'arm':\n",
    "            landmark_group = \"arm\"\n",
    "        elif view == \"side\" and ref_data['type'] == 'foot':\n",
    "            landmark_group = \"foot\"\n",
    "        \n",
    "        # Convert reference landmarks format\n",
    "        ref_landmarks = []\n",
    "        for lm_data in ref_data['landmarks']:\n",
    "            lm = mp_pose.PoseLandmark()\n",
    "            lm.x, lm.y, lm.z, lm.visibility = lm_data\n",
    "            ref_landmarks.append(lm)\n",
    "        \n",
    "        # Normalize both poses\n",
    "        norm_dancer = self.normalize_pose(dancer_landmarks)\n",
    "        norm_ref = self.normalize_pose(ref_landmarks)\n",
    "        \n",
    "        # Extract vectors for comparison\n",
    "        dancer_vector = self.extract_landmark_vectors(norm_dancer, landmark_group)\n",
    "        ref_vector = self.extract_landmark_vectors(norm_ref, landmark_group)\n",
    "        \n",
    "        # Calculate similarity score (cosine similarity)\n",
    "        similarity = cosine_similarity(dancer_vector, ref_vector)[0][0]\n",
    "        \n",
    "        # Scale to 0-100%\n",
    "        score = max(0, min(100, (similarity + 1) * 50))\n",
    "        \n",
    "        # Generate feedback message\n",
    "        message = self._generate_feedback(score, ref_pose_name, ref_data)\n",
    "        \n",
    "        return {\n",
    "            \"score\": score,\n",
    "            \"message\": message\n",
    "        }\n",
    "    \n",
    "    def _generate_feedback(self, score, pose_name, ref_data):\n",
    "        \"\"\"Generate feedback based on score\"\"\"\n",
    "        if score >= 90:\n",
    "            return f\"Excellent execution of {pose_name}! {ref_data['description']}\"\n",
    "        elif score >= 75:\n",
    "            return f\"Good execution of {pose_name}. Minor adjustments needed. {ref_data['description']}\"\n",
    "        elif score >= 60:\n",
    "            return f\"Fair execution of {pose_name}. Review the position details. {ref_data['description']}\"\n",
    "        else:\n",
    "            return f\"Needs improvement on {pose_name}. Focus on the key elements: {ref_data['description']}\"\n",
    "    \n",
    "    def analyze_dance_sequence(self, dancer_image_path, reference_sequence, view=\"front\"):\n",
    "        \"\"\"Analyze a dancer's execution of a sequence of poses\"\"\"\n",
    "        dancer_image, dancer_rgb, dancer_results = process_image(dancer_image_path)\n",
    "        \n",
    "        if not dancer_results.pose_landmarks:\n",
    "            return {\"error\": \"No pose detected in dancer image\"}\n",
    "        \n",
    "        # Visualize the detected pose\n",
    "        visualize_pose(dancer_rgb, dancer_results)\n",
    "        \n",
    "        # Compare with each reference pose in the sequence\n",
    "        results = {}\n",
    "        for pose_name in reference_sequence:\n",
    "            results[pose_name] = self.compare_poses(\n",
    "                dancer_results.pose_landmarks.landmark, \n",
    "                pose_name, \n",
    "                view\n",
    "            )\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3d515e-5025-4a37-a019-5e69d85eea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANCE EVALUATOR CLASS\n",
    "class DanceEvaluator:\n",
    "    \"\"\"Evaluates a complete dance performance and generates reports\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        \"\"\"Initialize the dance evaluator\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        \n",
    "    def evaluate_performance(self, dancer_frontal_image, dancer_side_image, arm_sequence, foot_sequence):\n",
    "        \"\"\"Evaluate a dance performance from both frontal and side views\"\"\"\n",
    "        # Analyze arm movements (frontal view)\n",
    "        print(\"Analyzing arm movements (frontal view)...\")\n",
    "        arm_results = self.analyzer.analyze_dance_sequence(\n",
    "            dancer_frontal_image, \n",
    "            arm_sequence, \n",
    "            view=\"front\"\n",
    "        )\n",
    "        \n",
    "        # Analyze footwork (side view)\n",
    "        print(\"Analyzing footwork (side view)...\")\n",
    "        foot_results = self.analyzer.analyze_dance_sequence(\n",
    "            dancer_side_image, \n",
    "            foot_sequence, \n",
    "            view=\"side\"\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        complete_evaluation = {\n",
    "            \"arm_movements\": arm_results,\n",
    "            \"footwork\": foot_results\n",
    "        }\n",
    "        \n",
    "        return complete_evaluation\n",
    "    \n",
    "    def generate_feedback_report(self, evaluation_results):\n",
    "        \"\"\"Generate a comprehensive feedback report\"\"\"\n",
    "        arm_results = evaluation_results.get(\"arm_movements\", {})\n",
    "        foot_results = evaluation_results.get(\"footwork\", {})\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        all_scores = []\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_scores.append(result[\"score\"])\n",
    "        \n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_scores.append(result[\"score\"])\n",
    "        \n",
    "        overall_score = np.mean(all_scores) if all_scores else 0\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html = f\"\"\"\n",
    "        <h2>Spanish Dance Performance Evaluation</h2>\n",
    "        <h3>Overall Score: {overall_score:.1f}%</h3>\n",
    "        \n",
    "        <h3>Arm Movements (Frontal View)</h3>\n",
    "        <table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Position</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Score</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Feedback</th>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add arm movement results\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                score = result[\"score\"]\n",
    "                message = result.get(\"message\", \"\")\n",
    "                \n",
    "                # Determine rating color based on score\n",
    "                if score >= 90:\n",
    "                    color = \"green\"\n",
    "                    rating = \"Excellent\"\n",
    "                elif score >= 75:\n",
    "                    color = \"#5cb85c\"  # lighter green\n",
    "                    rating = \"Good\"\n",
    "                elif score >= 60:\n",
    "                    color = \"orange\"\n",
    "                    rating = \"Fair\"\n",
    "                else:\n",
    "                    color = \"red\"\n",
    "                    rating = \"Needs Improvement\"\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\">{pose_name}</td>\n",
    "                    <td style=\"padding: 8px; color: {color};\">{score:.1f}% ({rating})</td>\n",
    "                    <td style=\"padding: 8px;\">{message}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Footwork (Side View)</h3>\n",
    "        <table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Position</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Score</th>\n",
    "            <th style=\"padding: 8px; text-align: left;\">Feedback</th>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add footwork results\n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                score = result[\"score\"]\n",
    "                message = result.get(\"message\", \"\")\n",
    "                \n",
    "                # Determine rating color based on score\n",
    "                if score >= 90:\n",
    "                    color = \"green\"\n",
    "                    rating = \"Excellent\"\n",
    "                elif score >= 75:\n",
    "                    color = \"#5cb85c\"  # lighter green\n",
    "                    rating = \"Good\"\n",
    "                elif score >= 60:\n",
    "                    color = \"orange\"\n",
    "                    rating = \"Fair\"\n",
    "                else:\n",
    "                    color = \"red\"\n",
    "                    rating = \"Needs Improvement\"\n",
    "                \n",
    "                html += f\"\"\"\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\">{pose_name}</td>\n",
    "                    <td style=\"padding: 8px; color: {color};\">{score:.1f}% ({rating})</td>\n",
    "                    <td style=\"padding: 8px;\">{message}</td>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Areas for Improvement</h3>\n",
    "        <p>Focus on the following positions:</p>\n",
    "        <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find lowest-scoring positions for improvement focus\n",
    "        all_results = []\n",
    "        for pose_name, result in arm_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_results.append((pose_name, result[\"score\"], \"arm\"))\n",
    "        \n",
    "        for pose_name, result in foot_results.items():\n",
    "            if isinstance(result, dict) and \"score\" in result:\n",
    "                all_results.append((pose_name, result[\"score\"], \"foot\"))\n",
    "        \n",
    "        # Sort by score (ascending) and take the 3 lowest\n",
    "        all_results.sort(key=lambda x: x[1])\n",
    "        for pose_name, score, pose_type in all_results[:3]:\n",
    "            html += f\"<li><strong>{pose_name}</strong> ({pose_type} position): {score:.1f}%</li>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "        </ul>\n",
    "        \n",
    "        <p>Practice these positions to improve your overall performance.</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Display the HTML report\n",
    "        display(HTML(html))\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def display_report(self, evaluation_results):\n",
    "        \"\"\"Display the feedback report\"\"\"\n",
    "        return self.generate_feedback_report(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86915d53-946b-44f6-8a11-6abc9e1e9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USAGE\n",
    "# Create the reference pose manager and add sample reference poses\n",
    "def setup_reference_dataset():\n",
    "    \"\"\"Set up a sample reference dataset\"\"\"\n",
    "    ref_manager = ReferencePoseManager(data_dir=\"spanish_dance_references\")\n",
    "    \n",
    "    # Example: Add reference poses for arm movements (frontal view)\n",
    "    arm_poses = {\n",
    "        \"brazos_en_quinta\": {\n",
    "            \"path\": \"dataset/arms/quinta.jpg\",\n",
    "            \"description\": \"Arms in fifth position, rounded above the head\"\n",
    "        },\n",
    "        \"braceo_flamenco\": {\n",
    "            \"path\": \"dataset/arms/braceo.jpg\",\n",
    "            \"description\": \"Flowing arm movement with wrists rotating\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, data in arm_poses.items():\n",
    "        ref_manager.add_reference_pose(name, data[\"path\"], \"arm\", data[\"description\"])\n",
    "    \n",
    "    # Example: Add reference poses for footwork (side view)\n",
    "    foot_poses = {\n",
    "        \"planta_tacón\": {\n",
    "            \"path\": \"dataset/feet/planta_tacon.jpg\",\n",
    "            \"description\": \"Heel-to-toe rhythmic step\"\n",
    "        },\n",
    "        \"golpe\": {\n",
    "            \"path\": \"dataset/feet/golpe.jpg\",\n",
    "            \"description\": \"Sharp stamp of the whole foot\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, data in foot_poses.items():\n",
    "        ref_manager.add_reference_pose(name, data[\"path\"], \"foot\", data[\"description\"])\n",
    "    \n",
    "    return ref_manager\n",
    "\n",
    "# Cell 8: Evaluate a dancer's performance\n",
    "def evaluate_dancer_performance(dancer_name, frontal_image_path, side_image_path):\n",
    "    \"\"\"Evaluate a dancer's performance\"\"\"\n",
    "    # Load the reference dataset\n",
    "    ref_manager = ReferencePoseManager(data_dir=\"spanish_dance_references\")\n",
    "    ref_manager.load_references()\n",
    "    \n",
    "    # Create the analyzer and evaluator\n",
    "    analyzer = PoseAnalyzer(ref_manager)\n",
    "    evaluator = DanceEvaluator(analyzer)\n",
    "    \n",
    "    # Define the sequence of positions to evaluate\n",
    "    arm_sequence = [\"brazos_en_quinta\", \"braceo_flamenco\"]\n",
    "    foot_sequence = [\"planta_tacón\", \"golpe\"]\n",
    "    \n",
    "    # Evaluate the dancer's performance\n",
    "    results = evaluator.evaluate_performance(\n",
    "        frontal_image_path,\n",
    "        side_image_path,\n",
    "        arm_sequence,\n",
    "        foot_sequence\n",
    "    )\n",
    "    \n",
    "    # Display the evaluation report\n",
    "    print(f\"Performance Evaluation for {dancer_name}\")\n",
    "    evaluator.display_report(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cell 9: Run evaluation on test data\n",
    "# Uncomment and run when ready\n",
    "# setup_reference_dataset()\n",
    "# evaluate_dancer_performance(\n",
    "#     \"Test Dancer\",\n",
    "#     \"test_data/dancer_front.jpg\",\n",
    "#     \"test_data/dancer_side.jpg\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9277c-3ccd-4032-abb2-73bcbfd4250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741270696.943663 2573922 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1741270696.946025 2568791 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3\n",
      "W0000 00:00:1741270696.984349 2573921 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741270697.000711 2573940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741270697.015037 2573939 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-03-06 15:18:53.383 python[44073:2568791] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-06 15:18:53.383 python[44073:2568791] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "#SPANISH DANCE POSE ANALYSIS TOOL\n",
    "\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Spanish Dance Pose Analysis Tool\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "# Function to load predefined images\n",
    "def load_predefined_images():\n",
    "    global predefined_images\n",
    "    file_paths = filedialog.askopenfilenames(title=\"Select Predefined Position Images\", filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg\")])\n",
    "    predefined_images = [cv2.imread(file) for file in file_paths]\n",
    "    messagebox.showinfo(\"Info\", f\"Loaded {len(predefined_images)} predefined images\")\n",
    "\n",
    "# Function to load exercise video\n",
    "def load_exercise_video():\n",
    "    global exercise_video_path\n",
    "    exercise_video_path = filedialog.askopenfilename(title=\"Select Exercise Video\", filetypes=[(\"Video Files\", \"*.mp4;*.avi;*.mov\")])\n",
    "    messagebox.showinfo(\"Info\", \"Exercise video loaded successfully\")\n",
    "\n",
    "# Function to start real-time camera observation\n",
    "def start_camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame using MediaPipe Pose\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                h, w, _ = frame.shape\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "        \n",
    "        cv2.imshow(\"Real-Time Pose Tracking\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to generate checklist and report\n",
    "def generate_report():\n",
    "    # Placeholder logic for evaluation\n",
    "    messagebox.showinfo(\"Report\", \"Checklist and report generated successfully!\")\n",
    "\n",
    "# GUI Buttons\n",
    "btn_load_images = tk.Button(root, text=\"Load Predefined Images\", command=load_predefined_images)\n",
    "btn_load_images.pack(pady=10)\n",
    "\n",
    "btn_load_video = tk.Button(root, text=\"Load Exercise Video\", command=load_exercise_video)\n",
    "btn_load_video.pack(pady=10)\n",
    "\n",
    "btn_start_camera = tk.Button(root, text=\"Start Camera Observation\", command=start_camera)\n",
    "btn_start_camera.pack(pady=10)\n",
    "\n",
    "btn_generate_report = tk.Button(root, text=\"Generate Report\", command=generate_report)\n",
    "btn_generate_report.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcvha-kernel",
   "language": "python",
   "name": "ipcvha-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
