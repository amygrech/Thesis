{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a84a88-e2ed-4c89-b486-4939a388c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python-headless in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/amygrech/.local/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python --user\n",
    "%pip install opencv-python-headless\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef1fcfa-f6b9-49fb-86ef-db689d692321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/amygrech/.local/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amygrech/.local/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/amygrech/.local/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amygrech/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6384e451-0096-43a9-b7b5-d614e6fe461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6dd709-18d8-4f77-8055-a28869e682a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 12:26:43.685 python[85413:25315152] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "2025-04-26 12:26:45.189 python[85413:25315152] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-26 12:26:45.190 python[85413:25315152] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Feed has been turned off.\n"
     ]
    }
   ],
   "source": [
    "# VIDEO FEED\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "    # Check if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Additional fix to forcefully kill stuck windows\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# Display exit message\n",
    "print(\"Camera Feed has been turned off.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c1dfc-94c4-407c-b2d7-a03f3a8d0f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 17:26:56.210 python[47271:3716605] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748878017.470850 3716605 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748878017.572867 3759018 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748878017.621039 3759024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748878017.679083 3759025 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Feed has been turned off.\n"
     ]
    }
   ],
   "source": [
    "## SETUP MEDIAPIPE INSTANCE USING CAMERA FEED\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Setup mediapipe instance with lower confidence thresholds\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.3,  # Lower threshold to catch more difficult poses\n",
    "    min_tracking_confidence=0.3,   # Lower tracking threshold\n",
    "    model_complexity=2,            # Use the most accurate model (0=Lite, 1=Medium, 2=Heavy)\n",
    "    smooth_landmarks=True          # Enable temporal filtering for stability\n",
    ") as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip the camera horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False  # Optimization for inference\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        if results.pose_landmarks:\n",
    "            # Draw full body connections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            \n",
    "            # Draw enhanced foot landmarks specifically\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            h, w, c = image.shape\n",
    "            \n",
    "            # Foot landmarks (left: 31,33, right: 32,34)\n",
    "            foot_landmarks = [\n",
    "                (31, \"Left Foot\"), (32, \"Right Foot\"),\n",
    "                (29, \"Left Heel\"), (30, \"Right Heel\"),\n",
    "                (27, \"Left Ankle\"), (28, \"Right Ankle\")\n",
    "            ]\n",
    "            \n",
    "            # Draw larger, more visible circles for feet\n",
    "            for idx, name in foot_landmarks:\n",
    "                if landmarks[idx].visibility > 0.5:  # Only if somewhat visible\n",
    "                    cx, cy = int(landmarks[idx].x * w), int(landmarks[idx].y * h)\n",
    "                    cv2.circle(image, (cx, cy), 8, (0, 255, 0), -1)  # Larger green circle\n",
    "                    cv2.putText(image, name, (cx-10, cy-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # Display confidence metric on screen\n",
    "        cv2.putText(image, 'Full Body Dance Tracking', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Add visibility indicator for feet\n",
    "        if results.pose_landmarks:\n",
    "            left_foot_visible = landmarks[31].visibility > 0.5\n",
    "            right_foot_visible = landmarks[32].visibility > 0.5\n",
    "            feet_status = f\"Feet detected: L: {'✓' if left_foot_visible else '✗'} R: {'✓' if right_foot_visible else '✗'}\"\n",
    "            cv2.putText(image, feet_status, (10, 60), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Dance Pose Tracking', image)\n",
    "\n",
    "        # Exit loop when 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Additional fix to ensure the window closes properly\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# Print exit message\n",
    "print(\"Camera Feed has been turned off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3dff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749136278.697260 5571348 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3\n",
      "W0000 00:00:1749136278.765424 5573550 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749136278.790644 5573549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video or failed to read frame.\n",
      "Video processing completed.\n"
     ]
    }
   ],
   "source": [
    "## SETUP MEDIAPIPE INSTANCE ON VIDEO\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Replace camera input with video file\n",
    "video_path = \"/Users/amygrech/Desktop/MCAST/Dissertation/Videos/Advanced/Fitted clothes Eliza.mov\" \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file: {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties for optional playback control\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_delay = int(1000 / fps) if fps > 0 else 33  # Delay between frames in milliseconds\n",
    "\n",
    "# Setup mediapipe instance with lower confidence thresholds\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.3,  # Lower threshold to catch more difficult poses\n",
    "    min_tracking_confidence=0.3,   # Lower tracking threshold\n",
    "    model_complexity=2,            # Use the most accurate model (0=Lite, 1=Medium, 2=Heavy)\n",
    "    smooth_landmarks=True          # Enable temporal filtering for stability\n",
    ") as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Remove the flip since it's a video file (not live camera)\n",
    "        # frame = cv2.flip(frame, 1)  # Comment out or remove this line\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False  # Optimization for inference\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        if results.pose_landmarks:\n",
    "            # Draw full body connections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            \n",
    "            # Draw enhanced foot landmarks specifically\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            h, w, c = image.shape\n",
    "            \n",
    "            # Foot landmarks (left: 31,33, right: 32,34)\n",
    "            foot_landmarks = [\n",
    "                (31, \"Left Foot\"), (32, \"Right Foot\"),\n",
    "                (29, \"Left Heel\"), (30, \"Right Heel\"),\n",
    "                (27, \"Left Ankle\"), (28, \"Right Ankle\")\n",
    "            ]\n",
    "            \n",
    "            # Draw larger, more visible circles for feet\n",
    "            for idx, name in foot_landmarks:\n",
    "                if landmarks[idx].visibility > 0.5:  # Only if somewhat visible\n",
    "                    cx, cy = int(landmarks[idx].x * w), int(landmarks[idx].y * h)\n",
    "                    cv2.circle(image, (cx, cy), 8, (0, 255, 0), -1)  # Larger green circle\n",
    "                    cv2.putText(image, name, (cx-10, cy-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # Display confidence metric on screen\n",
    "        cv2.putText(image, 'Full Body Dance Tracking - Video', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Add visibility indicator for feet\n",
    "        if results.pose_landmarks:\n",
    "            left_foot_visible = landmarks[31].visibility > 0.5\n",
    "            right_foot_visible = landmarks[32].visibility > 0.5\n",
    "            feet_status = f\"Feet detected: L: {'✓' if left_foot_visible else '✗'} R: {'✓' if right_foot_visible else '✗'}\"\n",
    "            cv2.putText(image, feet_status, (10, 60), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Dance Pose Tracking - Video', image)\n",
    "\n",
    "        # Use frame_delay for proper video playback speed, or press 'q' to quit\n",
    "        key = cv2.waitKey(frame_delay) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' '):  # Spacebar to pause/unpause\n",
    "            cv2.waitKey(0)  # Wait indefinitely until any key is pressed\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Additional fix to ensure the window closes properly\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# Print exit message\n",
    "print(\"Video processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b23681b-4e8a-4308-9147-d06a9b6dd999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mmp_drawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDrawingSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mthickness\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcircle_radius\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      DrawingSpec(color: Tuple[int, int, int] = (224, 224, 224), thickness: int = 2, circle_radius: int = 2)\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;34m@\u001b[0m\u001b[0mdataclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mDrawingSpec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Color for drawing the annotation. Default to the white color.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWHITE_COLOR\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Thickness for drawing the annotation. Default to 2 pixels.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mthickness\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# Circle radius. Default to 2 pixels.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0mcircle_radius\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.12/site-packages/mediapipe/python/solutions/drawing_utils.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "mp_drawing.DrawingSpec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16180260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749135880.844339 5568726 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1749135880.906923 5568930 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749135880.936151 5568934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-06-05 17:04:46.497 python[68792:5568726] The class 'NSOpenPanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Demonstration.mov\n",
      "Duration: 36.06 seconds, Total frames: 1803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749135888.233914 5568927 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 frames...\n",
      "Processed 200 frames...\n",
      "Processed 300 frames...\n",
      "Processed 400 frames...\n",
      "Processed 500 frames...\n",
      "Processed 600 frames...\n",
      "Processed 700 frames...\n",
      "Processed 800 frames...\n",
      "Processed 900 frames...\n",
      "Processed 1000 frames...\n",
      "Processed 1100 frames...\n",
      "Processed 1200 frames...\n",
      "Processed 1300 frames...\n",
      "Processed 1400 frames...\n",
      "Processed 1500 frames...\n",
      "Processed 1600 frames...\n",
      "Processed 1700 frames...\n",
      "Processed 1800 frames...\n",
      "Video processing complete. Extracted 901 poses.\n",
      "Processing video: Mara.mov\n",
      "Duration: 44.92 seconds, Total frames: 2246\n",
      "Processed 100 frames...\n",
      "Processed 200 frames...\n",
      "Processed 300 frames...\n",
      "Processed 400 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/amygrech/opt/anaconda3/lib/python3.12/tkinter/__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/amygrech/opt/anaconda3/lib/python3.12/tkinter/__init__.py\", line 861, in callit\n",
      "    func(*args)\n",
      "  File \"/var/folders/ty/jdtbwq2d4md3y64pnt393vsr0000gn/T/ipykernel_68792/30696729.py\", line 1616, in <lambda>\n",
      "    self.root.after(frame_delay, lambda: self.update_preview_videos(frame_delay))\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/ty/jdtbwq2d4md3y64pnt393vsr0000gn/T/ipykernel_68792/30696729.py\", line 1599, in update_preview_videos\n",
      "    for idx, name in foot_landmarks:\n",
      "                     ^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'foot_landmarks' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 frames...\n",
      "Processed 600 frames...\n",
      "Processed 700 frames...\n",
      "Processed 800 frames...\n",
      "Processed 900 frames...\n",
      "Processed 1000 frames...\n",
      "Processed 1100 frames...\n",
      "Processed 1200 frames...\n",
      "Processed 1300 frames...\n",
      "Processed 1400 frames...\n",
      "Processed 1500 frames...\n",
      "Processed 1600 frames...\n",
      "Processed 1700 frames...\n",
      "Processed 1800 frames...\n",
      "Processed 1900 frames...\n",
      "Processed 2000 frames...\n",
      "Processed 2100 frames...\n",
      "Processed 2200 frames...\n",
      "Video processing complete. Extracted 1089 poses.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "##PROTOTYPE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk, messagebox, simpledialog\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "from roboflow import Roboflow\n",
    "import json\n",
    "\n",
    "class GradeThresholds:\n",
    "    \"\"\"Define similarity thresholds for different Spanish dance grades\"\"\"\n",
    "    \n",
    "    GRADES = {\n",
    "        \"Grado Uno\": {\n",
    "            \"excellent\": 75,\n",
    "            \"good\": 65,\n",
    "            \"fair\": 55\n",
    "        },\n",
    "        \"Grado Dos\": {\n",
    "            \"excellent\": 80,\n",
    "            \"good\": 70,\n",
    "            \"fair\": 60\n",
    "        },\n",
    "        \"Grado Tres\": {\n",
    "            \"excellent\": 80,\n",
    "            \"good\": 70,\n",
    "            \"fair\": 60\n",
    "        },\n",
    "        \"Grado Quarto\": {\n",
    "            \"excellent\": 85,\n",
    "            \"good\": 75,\n",
    "            \"fair\": 65\n",
    "        },\n",
    "        \"Grado Cinco\": {\n",
    "            \"excellent\": 90,\n",
    "            \"good\": 75,\n",
    "            \"fair\": 65\n",
    "        },\n",
    "        \"Grado Seis\": {\n",
    "            \"excellent\": 90,\n",
    "            \"good\": 75,\n",
    "            \"fair\": 65\n",
    "        },\n",
    "        \"Grado Ocho\": {\n",
    "            \"excellent\": 90,\n",
    "            \"good\": 80,\n",
    "            \"fair\": 70\n",
    "        },\n",
    "        \"Advanced 1\": {\n",
    "            \"excellent\": 100,\n",
    "            \"good\": 98,\n",
    "            \"fair\": 95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_thresholds(cls, grade):\n",
    "        \"\"\"Get thresholds for specific grade\"\"\"\n",
    "        return cls.GRADES.get(grade, cls.GRADES[\"Grado Uno\"])\n",
    "    \n",
    "    @classmethod\n",
    "    def get_grade_names(cls):\n",
    "        \"\"\"Get list of all available grades\"\"\"\n",
    "        return list(cls.GRADES.keys())\n",
    "\n",
    "class SpanishDanceAnalyzer:\n",
    "    \"\"\"Main class to analyze Spanish dance performances using pose estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, use_roboflow=True, roboflow_api_key=None):\n",
    "        # MediaPipe setup\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # Roboflow setup for custom Spanish dance position detection\n",
    "        self.use_roboflow = use_roboflow\n",
    "        if use_roboflow and roboflow_api_key:\n",
    "            try:\n",
    "                rf = Roboflow(api_key=roboflow_api_key)\n",
    "                # Update with your actual workspace and project name\n",
    "                self.project = rf.workspace(\"ipcvhomeassignment-raaqp\").project(\"spanishposedetection\")\n",
    "                self.model = self.project.version(4).model\n",
    "                print(\"Roboflow model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load Roboflow model: {e}\")\n",
    "                self.use_roboflow = False\n",
    "        \n",
    "        # Define key landmarks for Spanish dance analysis\n",
    "        self.landmarks = {\n",
    "            # Upper body landmarks (for arm positions)\n",
    "            'left_wrist': self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            'right_wrist': self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            'left_elbow': self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            'right_elbow': self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            'left_shoulder': self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            'right_shoulder': self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            \n",
    "            # Lower body landmarks (for footwork)\n",
    "            'left_hip': self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            'right_hip': self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            'left_knee': self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            'right_knee': self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            'left_ankle': self.mp_pose.PoseLandmark.LEFT_ANKLE.value,\n",
    "            'right_ankle': self.mp_pose.PoseLandmark.RIGHT_ANKLE.value,\n",
    "            'left_foot_index': self.mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value,\n",
    "            'right_foot_index': self.mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value,\n",
    "        }\n",
    "        \n",
    "        # Landmark groups for specific analysis\n",
    "        self.arm_landmarks = [\n",
    "            'left_wrist', 'right_wrist', 'left_elbow', \n",
    "            'right_elbow', 'left_shoulder', 'right_shoulder'\n",
    "        ]\n",
    "        \n",
    "        self.foot_landmarks = [\n",
    "            'left_hip', 'right_hip', 'left_knee', 'right_knee',\n",
    "            'left_ankle', 'right_ankle', 'left_foot_index', 'right_foot_index'\n",
    "        ]\n",
    "        \n",
    "        # Storage for reference poses and choreography sequence\n",
    "        self.reference_poses = {}\n",
    "        self.choreography_sequence = []\n",
    "\n",
    "        # Keypoint comparison features\n",
    "        self.position_keypoints = {}  # Store reference keypoints for each position\n",
    "        self.use_keypoint_comparison = True  # Flag to enable/disable keypoint comparison\n",
    "        \n",
    "        # Data directory for saving/loading\n",
    "        self.data_dir = \"spanish_dance_data\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "        # Add grade-related attributes\n",
    "        self.current_grade = \"Grado Uno\"  # Default grade\n",
    "        self.grade_thresholds = GradeThresholds.get_thresholds(self.current_grade)\n",
    "    \n",
    "    def set_grade(self, grade):\n",
    "        \"\"\"Set the current grade and update thresholds\"\"\"\n",
    "        if grade in GradeThresholds.GRADES:\n",
    "            self.current_grade = grade\n",
    "            self.grade_thresholds = GradeThresholds.get_thresholds(grade)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_position_keypoints_from_roboflow(self, api_key):\n",
    "        \"\"\"\n",
    "        Load reference keypoints for each position from Roboflow\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            print(\"Error: API key required to load position keypoints\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Initialize Roboflow\n",
    "            rf = Roboflow(api_key=api_key)\n",
    "            \n",
    "            # Connect to your project\n",
    "            project = rf.workspace(\"ipcvhomeassignment-raaqp\").project(\"spanishposedetection\")\n",
    "            \n",
    "            # Get keypoints for each position\n",
    "            positions = [f\"position{i}\" for i in range(1, 16)]\n",
    "            keypoints_loaded = 0\n",
    "            \n",
    "            for position in positions:\n",
    "                try:\n",
    "                    # Use the dataset API to get annotations for this position\n",
    "                    annotations = project.version(4).annotations(class_name=position)\n",
    "                    \n",
    "                    if annotations and len(annotations) > 0:\n",
    "                        # Store the keypoints for this position\n",
    "                        keypoint_data = []\n",
    "                        for annotation in annotations:\n",
    "                            if 'keypoints' in annotation:\n",
    "                                keypoint_data.extend(annotation['keypoints'])\n",
    "                            elif isinstance(annotation, list):\n",
    "                                # Sometimes the API returns a list of keypoints directly\n",
    "                                keypoint_data.extend(annotation)\n",
    "                            elif isinstance(annotation, dict) and 'points' in annotation:\n",
    "                                # Or sometimes keypoints are in a 'points' key\n",
    "                                keypoint_data.extend(annotation['points'])\n",
    "                        \n",
    "                        if keypoint_data:\n",
    "                            self.position_keypoints[position] = keypoint_data\n",
    "                            print(f\"Loaded {len(keypoint_data)} keypoints for {position}\")\n",
    "                            keypoints_loaded += 1\n",
    "                    else:\n",
    "                        # Try alternate approach - get model predictions\n",
    "                        try:\n",
    "                            # Get a random example image for this position\n",
    "                            sample_images = project.version(4).examples(class_name=position, limit=1)\n",
    "                            if sample_images and len(sample_images) > 0:\n",
    "                                sample_url = sample_images[0].get('image', {}).get('url')\n",
    "                                if sample_url:\n",
    "                                    # Get prediction with keypoints\n",
    "                                    prediction = project.version(4).model.predict(sample_url, confidence=40, overlap=30).json()\n",
    "                                    \n",
    "                                    if 'predictions' in prediction and len(prediction['predictions']) > 0:\n",
    "                                        for pred in prediction['predictions']:\n",
    "                                            if pred.get('class') == position and 'keypoints' in pred:\n",
    "                                                self.position_keypoints[position] = pred['keypoints']\n",
    "                                                print(f\"Loaded keypoints for {position} from prediction\")\n",
    "                                                keypoints_loaded += 1\n",
    "                                else:\n",
    "                                    print(f\"No sample image URL found for {position}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error getting prediction for {position}: {e}\")\n",
    "                            \n",
    "                        if position not in self.position_keypoints:\n",
    "                            print(f\"No annotations found for {position}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading keypoints for {position}: {e}\")\n",
    "            \n",
    "            print(f\"Loaded keypoints for {keypoints_loaded} positions\")\n",
    "            # Enable keypoint comparison if we loaded any keypoints\n",
    "            self.use_keypoint_comparison = keypoints_loaded > 0\n",
    "            return keypoints_loaded > 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Roboflow: {e}\")\n",
    "            return False\n",
    "        \n",
    "    def extract_poses_from_video(self, video_path, skip_frames=2):\n",
    "        \"\"\"Process video and extract pose landmarks for each frame\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video: {video_path}\")\n",
    "            return [], [], None\n",
    "            \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        \n",
    "        print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds, Total frames: {frame_count}\")\n",
    "        \n",
    "        poses = []\n",
    "        frames = []\n",
    "        keyframes = []  # Store key frames for choreography points\n",
    "        processed_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            processed_count += 1\n",
    "            if processed_count % skip_frames != 0:\n",
    "                continue\n",
    "                \n",
    "            # Convert to RGB for MediaPipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process with MediaPipe\n",
    "            results = self.pose.process(rgb_frame)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Store frame and pose data\n",
    "                frames.append(frame)\n",
    "                \n",
    "                # Extract key landmarks\n",
    "                landmarks_dict = {}\n",
    "                for name, idx in self.landmarks.items():\n",
    "                    lm = results.pose_landmarks.landmark[idx]\n",
    "                    landmarks_dict[name] = {\n",
    "                        'x': lm.x, 'y': lm.y, 'z': lm.z, 'visibility': lm.visibility\n",
    "                    }\n",
    "                \n",
    "                pose_data = {\n",
    "                    'frame_idx': processed_count,\n",
    "                    'timestamp': processed_count / fps if fps > 0 else 0,\n",
    "                    'landmarks': landmarks_dict,\n",
    "                    'full_pose': results.pose_landmarks\n",
    "                }\n",
    "                \n",
    "                poses.append(pose_data)\n",
    "                \n",
    "                # Every 30 frames (adjusted by skip_frames), save a keyframe for reference\n",
    "                if processed_count % (30 * skip_frames) == 0:\n",
    "                    # Draw landmarks on frame for visualization\n",
    "                    annotated_frame = frame.copy()\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        annotated_frame, \n",
    "                        results.pose_landmarks,\n",
    "                        self.mp_pose.POSE_CONNECTIONS\n",
    "                    )\n",
    "                    keyframes.append({\n",
    "                        'frame_idx': processed_count,\n",
    "                        'timestamp': processed_count / fps if fps > 0 else 0,\n",
    "                        'frame': annotated_frame\n",
    "                    })\n",
    "            \n",
    "            # Show progress\n",
    "            if processed_count % 100 == 0:\n",
    "                print(f\"Processed {processed_count} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"Video processing complete. Extracted {len(poses)} poses.\")\n",
    "        return poses, frames, keyframes\n",
    "    \n",
    "    def normalize_pose_data(self, landmarks_dict):\n",
    "        \"\"\"Normalize pose data to make it scale and position invariant\"\"\"\n",
    "        if not landmarks_dict:\n",
    "            return None\n",
    "            \n",
    "        # Extract hip center coordinates\n",
    "        left_hip = np.array([\n",
    "            landmarks_dict['left_hip']['x'],\n",
    "            landmarks_dict['left_hip']['y'],\n",
    "            landmarks_dict['left_hip']['z']\n",
    "        ])\n",
    "        \n",
    "        right_hip = np.array([\n",
    "            landmarks_dict['right_hip']['x'],\n",
    "            landmarks_dict['right_hip']['y'],\n",
    "            landmarks_dict['right_hip']['z']\n",
    "        ])\n",
    "        \n",
    "        hip_center = (left_hip + right_hip) / 2\n",
    "        \n",
    "        # Calculate torso height for normalization\n",
    "        left_shoulder = np.array([\n",
    "            landmarks_dict['left_shoulder']['x'],\n",
    "            landmarks_dict['left_shoulder']['y'],\n",
    "            landmarks_dict['left_shoulder']['z']\n",
    "        ])\n",
    "        \n",
    "        right_shoulder = np.array([\n",
    "            landmarks_dict['right_shoulder']['x'],\n",
    "            landmarks_dict['right_shoulder']['y'],\n",
    "            landmarks_dict['right_shoulder']['z']\n",
    "        ])\n",
    "        \n",
    "        shoulder_center = (left_shoulder + right_shoulder) / 2\n",
    "        torso_height = np.linalg.norm(shoulder_center - hip_center)\n",
    "        \n",
    "        # Normalize landmarks relative to hip center and torso height\n",
    "        normalized_landmarks = {}\n",
    "        for name, lm in landmarks_dict.items():\n",
    "            point = np.array([lm['x'], lm['y'], lm['z']])\n",
    "            # Translate to make hip center the origin\n",
    "            normalized = point - hip_center\n",
    "            # Scale by torso height\n",
    "            if torso_height > 0:\n",
    "                normalized = normalized / torso_height\n",
    "            \n",
    "            normalized_landmarks[name] = {\n",
    "                'x': normalized[0],\n",
    "                'y': normalized[1],\n",
    "                'z': normalized[2],\n",
    "                'visibility': lm['visibility']\n",
    "            }\n",
    "        \n",
    "        return normalized_landmarks\n",
    "    \n",
    "    def normalize_all_poses(self, poses):\n",
    "        \"\"\"Normalize an array of poses\"\"\"\n",
    "        normalized_poses = []\n",
    "        for pose in poses:\n",
    "            norm_landmarks = self.normalize_pose_data(pose['landmarks'])\n",
    "            if norm_landmarks:\n",
    "                normalized_pose = pose.copy()\n",
    "                normalized_pose['normalized_landmarks'] = norm_landmarks\n",
    "                normalized_poses.append(normalized_pose)\n",
    "        \n",
    "        return normalized_poses\n",
    "    \n",
    "    def compare_with_keypoints(self, student_landmarks, position_name):\n",
    "        \"\"\"\n",
    "        Compare student landmarks with the reference keypoints for a specific position\n",
    "        \"\"\"\n",
    "        if not self.use_keypoint_comparison:\n",
    "            print(f\"Keypoint comparison disabled, falling back to default comparison\")\n",
    "            return None\n",
    "            \n",
    "        if position_name not in self.position_keypoints:\n",
    "            print(f\"No keypoints found for {position_name}, falling back to default comparison\")\n",
    "            return None\n",
    "            \n",
    "        # Get reference keypoints for this position\n",
    "        reference_keypoints = self.position_keypoints[position_name]\n",
    "        \n",
    "        if not reference_keypoints or len(reference_keypoints) == 0:\n",
    "            print(f\"Empty keypoints for {position_name}, falling back to default comparison\")\n",
    "            return None\n",
    "        \n",
    "        # Extract equivalent points from student landmarks\n",
    "        # Map common keypoint names to MediaPipe landmark indices\n",
    "        keypoint_map = {\n",
    "            \"left_wrist\": self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            \"right_wrist\": self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            \"left_elbow\": self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            \"right_elbow\": self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            \"left_shoulder\": self.mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            \"right_shoulder\": self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            \"left_hip\": self.mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            \"right_hip\": self.mp_pose.PoseLandmark.RIGHT_HIP.value,\n",
    "            \"left_knee\": self.mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            \"right_knee\": self.mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            \"left_ankle\": self.mp_pose.PoseLandmark.LEFT_ANKLE.value,\n",
    "            \"right_ankle\": self.mp_pose.PoseLandmark.RIGHT_ANKLE.value,\n",
    "            \"left_foot_index\": self.mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value,\n",
    "            \"right_foot_index\": self.mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value,\n",
    "            # Additional mappings for common pose keypoint names\n",
    "            \"left_hand\": self.mp_pose.PoseLandmark.LEFT_WRIST.value,\n",
    "            \"right_hand\": self.mp_pose.PoseLandmark.RIGHT_WRIST.value,\n",
    "            \"left_arm\": self.mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            \"right_arm\": self.mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "        }\n",
    "        \n",
    "        # Calculate similarity between reference and student keypoints\n",
    "        total_distance = 0\n",
    "        valid_keypoints = 0\n",
    "        \n",
    "        for kp in reference_keypoints:\n",
    "            # Check for different keypoint formats and extract name\n",
    "            kp_name = None\n",
    "            if isinstance(kp, dict):\n",
    "                if 'name' in kp:\n",
    "                    kp_name = kp.get('name')\n",
    "                elif 'label' in kp:\n",
    "                    kp_name = kp.get('label')\n",
    "                \n",
    "                # If no name field, try to infer from other fields\n",
    "                if not kp_name and ('x' in kp and 'y' in kp):\n",
    "                    # Try to match with closest landmark in student pose\n",
    "                    continue  # Skip for now, we'll implement a matching algorithm if needed\n",
    "            \n",
    "            if not kp_name:\n",
    "                continue\n",
    "                \n",
    "            # Clean up keypoint name (remove any prefix/suffix)\n",
    "            kp_name = kp_name.lower().strip()\n",
    "            if '_keypoint' in kp_name:\n",
    "                kp_name = kp_name.replace('_keypoint', '')\n",
    "            \n",
    "            if kp_name in keypoint_map:\n",
    "                # Get reference coordinates\n",
    "                ref_x, ref_y = None, None\n",
    "                \n",
    "                if isinstance(kp, dict):\n",
    "                    if 'x' in kp and 'y' in kp:\n",
    "                        ref_x, ref_y = kp.get('x') / 100.0, kp.get('y') / 100.0  # Normalize to 0-1 range\n",
    "                    elif 'position' in kp and isinstance(kp['position'], dict):\n",
    "                        pos = kp['position']\n",
    "                        if 'x' in pos and 'y' in pos:\n",
    "                            ref_x, ref_y = pos.get('x') / 100.0, pos.get('y') / 100.0\n",
    "                \n",
    "                # Skip if we couldn't extract coordinates\n",
    "                if ref_x is None or ref_y is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get corresponding student landmark\n",
    "                student_idx = keypoint_map[kp_name]\n",
    "                student_lm = student_landmarks.landmark[student_idx]\n",
    "                \n",
    "                # Calculate distance (normalized by image dimensions)\n",
    "                distance = ((student_lm.x - ref_x) ** 2 + (student_lm.y - ref_y) ** 2) ** 0.5\n",
    "                total_distance += distance\n",
    "                valid_keypoints += 1\n",
    "                \n",
    "                # Debug: print keypoint comparison\n",
    "                # print(f\"Comparing {kp_name}: ref({ref_x:.2f}, {ref_y:.2f}) vs student({student_lm.x:.2f}, {student_lm.y:.2f}) = {distance:.4f}\")\n",
    "        \n",
    "        if valid_keypoints == 0:\n",
    "            print(f\"No valid keypoints matched for {position_name}\")\n",
    "            return None\n",
    "            \n",
    "        # Average distance (lower is better)\n",
    "        avg_distance = total_distance / valid_keypoints\n",
    "        \n",
    "        # Convert to similarity score (0-100%)\n",
    "        # The closer the distance is to 0, the higher the similarity\n",
    "        similarity = max(0, 100 * (1 - min(avg_distance * 5, 1)))\n",
    "        \n",
    "        print(f\"Keypoint comparison for {position_name}: {similarity:.2f}% (from {valid_keypoints} keypoints)\")\n",
    "        return similarity\n",
    "    \n",
    "    def evaluate_position(self, landmarks, position_name):\n",
    "        \"\"\"\n",
    "        Evaluate how closely the current pose matches a specific dance position\n",
    "        \"\"\"\n",
    "        # Print status of keypoint comparison\n",
    "        if not self.use_keypoint_comparison:\n",
    "            print(f\"Keypoint comparison is disabled for {position_name}\")\n",
    "        elif position_name not in self.position_keypoints:\n",
    "            print(f\"No keypoints found for {position_name}\")\n",
    "        elif not self.position_keypoints[position_name]:\n",
    "            print(f\"Empty keypoints array for {position_name}\")\n",
    "        \n",
    "        # First try with keypoint comparison if enabled\n",
    "        if self.use_keypoint_comparison:\n",
    "            keypoint_similarity = self.compare_with_keypoints(landmarks, position_name)\n",
    "            if keypoint_similarity is not None:\n",
    "                return keypoint_similarity\n",
    "        \n",
    "        # Fall back to a more improved default method\n",
    "        # This is just a placeholder - you can implement your own logic based on pose angles\n",
    "        try:\n",
    "            # Get key landmarks from the pose\n",
    "            lm = landmarks.landmark\n",
    "            \n",
    "            # Check for arm positions (example for position1 - arms in circle above head)\n",
    "            if position_name == \"position1\":\n",
    "                # Measure arm position - for position1, we want arms raised in a circle above head\n",
    "                left_wrist_y = lm[self.mp_pose.PoseLandmark.LEFT_WRIST.value].y\n",
    "                right_wrist_y = lm[self.mp_pose.PoseLandmark.RIGHT_WRIST.value].y\n",
    "                left_shoulder_y = lm[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value].y\n",
    "                right_shoulder_y = lm[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y\n",
    "                \n",
    "                # Arms above shoulders?\n",
    "                arms_raised = (left_wrist_y < left_shoulder_y) and (right_wrist_y < right_shoulder_y)\n",
    "                \n",
    "                # Score based on arm position\n",
    "                if arms_raised:\n",
    "                    return 75.0  # Good match\n",
    "                else:\n",
    "                    return 50.0  # Poor match\n",
    "            \n",
    "            # For other positions, we'll use the generic fallback\n",
    "            # Different fallback values based on position number for variety\n",
    "            position_num = 0\n",
    "            if position_name.startswith(\"position\"):\n",
    "                try:\n",
    "                    position_num = int(position_name[8:])\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            # Base score on position number (just for demonstration)\n",
    "            base_score = 60 + (position_num % 10) * 2\n",
    "            \n",
    "            # Add some variability based on frame landmarks\n",
    "            import time\n",
    "            variability = (hash(str(time.time())) % 20) - 10  # -10 to +10 range\n",
    "            \n",
    "            return max(50, min(95, base_score + variability))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in position evaluation: {e}\")\n",
    "            import random\n",
    "            return random.uniform(50, 95)  # Fallback to random as in original code\n",
    "\n",
    "    \n",
    "    def set_reference_demo(self, video_path):\n",
    "        \"\"\"Set the demonstration video as reference for analysis\"\"\"\n",
    "        poses, frames, keyframes = self.extract_poses_from_video(video_path)\n",
    "        self.reference_poses = self.normalize_all_poses(poses)\n",
    "        self.reference_keyframes = keyframes\n",
    "        \n",
    "        # Save reference data\n",
    "        self.save_reference_data()\n",
    "        \n",
    "        return len(self.reference_poses)\n",
    "    \n",
    "    def save_reference_data(self):\n",
    "        \"\"\"Save reference pose data to disk\"\"\"\n",
    "        # We don't save full frames to save disk space\n",
    "        serializable_poses = []\n",
    "        for pose in self.reference_poses:\n",
    "            pose_copy = pose.copy()\n",
    "            # Remove non-serializable objects\n",
    "            if 'full_pose' in pose_copy:\n",
    "                del pose_copy['full_pose']\n",
    "            serializable_poses.append(pose_copy)\n",
    "        \n",
    "        with open(os.path.join(self.data_dir, \"reference_poses.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(serializable_poses, f)\n",
    "        \n",
    "        # Save keyframes as images\n",
    "        keyframes_dir = os.path.join(self.data_dir, \"keyframes\")\n",
    "        os.makedirs(keyframes_dir, exist_ok=True)\n",
    "        \n",
    "        keyframe_data = []\n",
    "        for i, kf in enumerate(self.reference_keyframes):\n",
    "            filename = f\"keyframe_{i:03d}.jpg\"\n",
    "            filepath = os.path.join(keyframes_dir, filename)\n",
    "            cv2.imwrite(filepath, kf['frame'])\n",
    "            \n",
    "            keyframe_data.append({\n",
    "                'frame_idx': kf['frame_idx'],\n",
    "                'timestamp': kf['timestamp'],\n",
    "                'filename': filename\n",
    "            })\n",
    "        \n",
    "        with open(os.path.join(self.data_dir, \"keyframes.json\"), \"w\") as f:\n",
    "            json.dump(keyframe_data, f)\n",
    "    \n",
    "    def load_reference_data(self):\n",
    "        \"\"\"Load reference pose data from disk\"\"\"\n",
    "        ref_file = os.path.join(self.data_dir, \"reference_poses.pkl\")\n",
    "        if os.path.exists(ref_file):\n",
    "            with open(ref_file, \"rb\") as f:\n",
    "                self.reference_poses = pickle.load(f)\n",
    "            \n",
    "            # Load keyframe data\n",
    "            keyframes_data_file = os.path.join(self.data_dir, \"keyframes.json\")\n",
    "            keyframes_dir = os.path.join(self.data_dir, \"keyframes\")\n",
    "            \n",
    "            if os.path.exists(keyframes_data_file):\n",
    "                with open(keyframes_data_file, \"r\") as f:\n",
    "                    keyframe_data = json.load(f)\n",
    "                \n",
    "                self.reference_keyframes = []\n",
    "                for kf in keyframe_data:\n",
    "                    filepath = os.path.join(keyframes_dir, kf['filename'])\n",
    "                    if os.path.exists(filepath):\n",
    "                        frame = cv2.imread(filepath)\n",
    "                        self.reference_keyframes.append({\n",
    "                            'frame_idx': kf['frame_idx'],\n",
    "                            'timestamp': kf['timestamp'],\n",
    "                            'frame': frame\n",
    "                        })\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def define_choreography_sequence(self, timestamps, labels):\n",
    "        \"\"\"Define key moments in the choreography with labels\"\"\"\n",
    "        self.choreography_sequence = []\n",
    "        \n",
    "        for timestamp, label in zip(timestamps, labels):\n",
    "            # Find the closest pose to the timestamp\n",
    "            closest_pose = None\n",
    "            min_diff = float('inf')\n",
    "            \n",
    "            for pose in self.reference_poses:\n",
    "                diff = abs(pose['timestamp'] - timestamp)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    closest_pose = pose\n",
    "            \n",
    "            if closest_pose:\n",
    "                self.choreography_sequence.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'label': label,\n",
    "                    'pose_idx': closest_pose['frame_idx']\n",
    "                })\n",
    "        \n",
    "        # Save choreography sequence\n",
    "        with open(os.path.join(self.data_dir, \"choreography.json\"), \"w\") as f:\n",
    "            json.dump(self.choreography_sequence, f)\n",
    "        \n",
    "        return len(self.choreography_sequence)\n",
    "    \n",
    "    def load_choreography_sequence(self):\n",
    "        \"\"\"Load choreography sequence from disk\"\"\"\n",
    "        choreo_file = os.path.join(self.data_dir, \"choreography.json\")\n",
    "        if os.path.exists(choreo_file):\n",
    "            with open(choreo_file, \"r\") as f:\n",
    "                self.choreography_sequence = json.load(f)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def extract_features(self, landmarks_dict, landmark_group=\"all\"):\n",
    "        \"\"\"Extract pose features for comparison\"\"\"\n",
    "        if landmark_group == \"arms\":\n",
    "            target_landmarks = self.arm_landmarks\n",
    "        elif landmark_group == \"feet\":\n",
    "            target_landmarks = self.foot_landmarks\n",
    "        else:\n",
    "            target_landmarks = list(self.landmarks.keys())\n",
    "        \n",
    "        features = []\n",
    "        for name in target_landmarks:\n",
    "            if name in landmarks_dict:\n",
    "                lm = landmarks_dict[name]\n",
    "                features.extend([lm['x'], lm['y'], lm['z']])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def compute_pose_similarity(self, pose1, pose2, landmark_group=\"all\"):\n",
    "        \"\"\"Compute similarity between two poses\"\"\"\n",
    "        # Extract features from normalized landmarks\n",
    "        features1 = self.extract_features(pose1['normalized_landmarks'], landmark_group)\n",
    "        features2 = self.extract_features(pose2['normalized_landmarks'], landmark_group)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(features1.reshape(1, -1), features2.reshape(1, -1))[0][0]\n",
    "        \n",
    "        # Normalize to percentage (0-100%)\n",
    "        similarity_percent = (similarity + 1) * 50\n",
    "        \n",
    "        return similarity_percent\n",
    "    \n",
    "    def find_matching_sequence(self, student_poses, window_size=3):\n",
    "        \"\"\"Find the best matching sequence in student video for each choreography point\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for choreo_point in self.choreography_sequence:\n",
    "            # Find reference pose for this choreography point\n",
    "            ref_pose_idx = None\n",
    "            for i, pose in enumerate(self.reference_poses):\n",
    "                if pose['frame_idx'] == choreo_point['pose_idx']:\n",
    "                    ref_pose_idx = i\n",
    "                    break\n",
    "            \n",
    "            if ref_pose_idx is None:\n",
    "                continue\n",
    "            \n",
    "            ref_pose = self.reference_poses[ref_pose_idx]\n",
    "            \n",
    "            # Find best matching pose in student video\n",
    "            best_match_idx = -1\n",
    "            best_similarity = 0\n",
    "            \n",
    "            for i, student_pose in enumerate(student_poses):\n",
    "                # Check if we have enough window space\n",
    "                if i < len(student_poses) - window_size:\n",
    "                    # Compute average similarity over a window of poses\n",
    "                    window_similarity = 0\n",
    "                    for w in range(window_size):\n",
    "                        pose_similarity = self.compute_pose_similarity(\n",
    "                            ref_pose, student_poses[i + w]\n",
    "                        )\n",
    "                        window_similarity += pose_similarity\n",
    "                    \n",
    "                    window_similarity /= window_size\n",
    "                    \n",
    "                    if window_similarity > best_similarity:\n",
    "                        best_similarity = window_similarity\n",
    "                        best_match_idx = i\n",
    "            \n",
    "            # Determine arm and footwork specific similarity\n",
    "            arm_similarity = 0\n",
    "            foot_similarity = 0\n",
    "            \n",
    "            if best_match_idx >= 0:\n",
    "                best_student_pose = student_poses[best_match_idx]\n",
    "                arm_similarity = self.compute_pose_similarity(ref_pose, best_student_pose, \"arms\")\n",
    "                foot_similarity = self.compute_pose_similarity(ref_pose, best_student_pose, \"feet\")\n",
    "            \n",
    "            results.append({\n",
    "                'choreography_point': choreo_point,\n",
    "                'label': choreo_point['label'],\n",
    "                'reference_pose_idx': ref_pose_idx,\n",
    "                'student_pose_idx': best_match_idx,\n",
    "                'overall_similarity': best_similarity,\n",
    "                'arm_similarity': arm_similarity,\n",
    "                'foot_similarity': foot_similarity\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_student_performance(self, student_video_path):\n",
    "        \"\"\"Analyze a student's dance performance compared to reference\"\"\"\n",
    "        # Make sure we have reference data\n",
    "        if not self.reference_poses:\n",
    "            if not self.load_reference_data():\n",
    "                print(\"Error: No reference data available. Please set a reference demo first.\")\n",
    "                return None\n",
    "        \n",
    "        if not self.choreography_sequence:\n",
    "            if not self.load_choreography_sequence():\n",
    "                print(\"Error: No choreography sequence defined. Please define choreography first.\")\n",
    "                return None\n",
    "        \n",
    "        # Process student video\n",
    "        student_poses, _, student_keyframes = self.extract_poses_from_video(student_video_path)\n",
    "        normalized_student_poses = self.normalize_all_poses(student_poses)\n",
    "        \n",
    "        # Find matching sequences\n",
    "        matching_results = self.find_matching_sequence(normalized_student_poses)\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        overall_score = 0\n",
    "        arm_score = 0\n",
    "        foot_score = 0\n",
    "        \n",
    "        if matching_results:\n",
    "            overall_score = sum(r['overall_similarity'] for r in matching_results) / len(matching_results)\n",
    "            arm_score = sum(r['arm_similarity'] for r in matching_results) / len(matching_results)\n",
    "            foot_score = sum(r['foot_similarity'] for r in matching_results) / len(matching_results)\n",
    "        \n",
    "        performance_analysis = {\n",
    "            'student_video': student_video_path,\n",
    "            'poses_extracted': len(student_poses),\n",
    "            'matching_results': matching_results,\n",
    "            'overall_score': overall_score,\n",
    "            'arm_score': arm_score,\n",
    "            'foot_score': foot_score\n",
    "        }\n",
    "        \n",
    "        return performance_analysis\n",
    "    \n",
    "    def analyze_dance_positions(self, landmarks, frame=None):\n",
    "        \"\"\"\n",
    "        Analyze the current pose landmarks and determine which dance positions are detected\n",
    "        \n",
    "        Args:\n",
    "            landmarks: The pose landmarks from MediaPipe\n",
    "            frame: Optional frame for visualization\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of position scores {position_name: confidence_score}\n",
    "        \"\"\"\n",
    "        position_scores = {}\n",
    "        \n",
    "        # Process standard positions (1-9, 15)\n",
    "        for i in range(1, 10):\n",
    "            position_name = f\"position{i}\"\n",
    "            score = self.evaluate_position(landmarks, position_name)\n",
    "            position_scores[position_name] = score\n",
    "        \n",
    "        position_scores[\"position15\"] = self.evaluate_position(landmarks, \"position15\")\n",
    "        \n",
    "        # Handle footwork positions (10-14)\n",
    "        footwork_positions = [f\"position{i}\" for i in range(10, 15)]\n",
    "        \n",
    "        # Check if any footwork positions are in our target sequence\n",
    "        examining_footwork = False\n",
    "        if hasattr(self, 'choreography_sequence'):\n",
    "            examining_footwork = any(pos['label'] in footwork_positions for pos in self.choreography_sequence)\n",
    "        \n",
    "        if examining_footwork:\n",
    "            # Only evaluate footwork positions if they're part of the target sequence\n",
    "            for position_name in footwork_positions:\n",
    "                score = self.evaluate_position(landmarks, position_name)\n",
    "                position_scores[position_name] = score\n",
    "        else:\n",
    "            # Mark all footwork positions as 0 if not being examined\n",
    "            for position_name in footwork_positions:\n",
    "                position_scores[position_name] = 0.0\n",
    "        \n",
    "        return position_scores\n",
    "    \n",
    "    def generate_feedback(self, performance_analysis):\n",
    "        \"\"\"Generate detailed feedback from performance analysis\"\"\"\n",
    "        if not performance_analysis:\n",
    "            return \"No performance analysis data available.\"\n",
    "        \n",
    "        overall_score = performance_analysis['overall_score']\n",
    "        arm_score = performance_analysis['arm_score']\n",
    "        foot_score = performance_analysis['foot_score']\n",
    "        \n",
    "        results = performance_analysis['matching_results']\n",
    "        \n",
    "        # Get thresholds for current grade\n",
    "        thresholds = self.grade_thresholds\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html = f\"\"\"\n",
    "        <h2>Spanish Dance Performance Evaluation - {self.current_grade}</h2>\n",
    "        <h3>Overall Technique Mark: {overall_score:.1f}%</h3>\n",
    "        <div style=\"margin-bottom: 20px;\">\n",
    "            <div style=\"display: inline-block; margin-right: 20px;\">\n",
    "                <strong>Arm Movements:</strong> {arm_score:.1f}%\n",
    "            </div>\n",
    "            <div style=\"display: inline-block;\">\n",
    "                <strong>Footwork:</strong> {foot_score:.1f}%\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h3>Choreography Evaluation</h3>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; margin-bottom: 20px;\">\n",
    "            <tr style=\"background-color: #f2f2f2;\">\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Position/Step</th>\n",
    "                <th style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">Execution</th>\n",
    "                <th style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">Similarity</th>\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Feedback</th>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add rows for each choreography point\n",
    "        for i, result in enumerate(results):\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            arm_sim = result['arm_similarity']\n",
    "            foot_sim = result['foot_similarity']\n",
    "            \n",
    "            # Determine execution rating using grade-specific thresholds\n",
    "            if similarity >= thresholds[\"excellent\"]:\n",
    "                execution = \"Excellent\"\n",
    "                color = \"green\"\n",
    "                feedback = f\"Excellent execution of {label}. Perfect arm position ({arm_sim:.1f}%) and footwork ({foot_sim:.1f}%).\"\n",
    "            elif similarity >= thresholds[\"good\"]:\n",
    "                execution = \"Good\"\n",
    "                color = \"#5cb85c\"  # lighter green\n",
    "                feedback = f\"Good execution of {label}. \"\n",
    "                if arm_sim < thresholds[\"good\"]:\n",
    "                    feedback += f\"Focus on improving arm positions ({arm_sim:.1f}%). \"\n",
    "                if foot_sim < thresholds[\"good\"]:\n",
    "                    feedback += f\"Pay attention to footwork details ({foot_sim:.1f}%). \"\n",
    "            elif similarity >= thresholds[\"fair\"]:\n",
    "                execution = \"Fair\"\n",
    "                color = \"orange\"\n",
    "                feedback = f\"Fair execution of {label}. \"\n",
    "                if arm_sim < foot_sim:\n",
    "                    feedback += f\"Arm positions need more practice ({arm_sim:.1f}%). \"\n",
    "                else:\n",
    "                    feedback += f\"Footwork needs refinement ({foot_sim:.1f}%). \"\n",
    "            else:\n",
    "                execution = \"Needs Improvement\"\n",
    "                color = \"red\"\n",
    "                feedback = f\"The {label} position needs significant improvement. Review the demonstration video carefully.\"\n",
    "            \n",
    "            # Add table row\n",
    "            html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"padding: 8px; border: 1px solid #ddd;\">{label}</td>\n",
    "                <td style=\"padding: 8px; text-align: center; color: {color}; border: 1px solid #ddd;\">{execution}</td>\n",
    "                <td style=\"padding: 8px; text-align: center; border: 1px solid #ddd;\">{similarity:.1f}%</td>\n",
    "                <td style=\"padding: 8px; border: 1px solid #ddd;\">{feedback}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Close table and add summary\n",
    "        html += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h3>Summary Feedback</h3>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate overall feedback based on scores and grade thresholds\n",
    "        if overall_score >= thresholds[\"excellent\"]:\n",
    "            html += f\"\"\"\n",
    "            <p>Outstanding performance for {self.current_grade}! Your technique closely matches the demonstration video. \n",
    "            Continue refining the minor details for even greater precision in your Spanish dance execution.</p>\n",
    "            \"\"\"\n",
    "        elif overall_score >= thresholds[\"good\"]:\n",
    "            html += f\"\"\"\n",
    "            <p>Good performance with strong technical foundations for {self.current_grade}. Focus on the positions where your similarity\n",
    "            score was lower, and pay attention to the precise arm positions and footwork timing.</p>\n",
    "            \"\"\"\n",
    "        elif overall_score >= thresholds[\"fair\"]:\n",
    "            html += f\"\"\"\n",
    "            <p>Fair performance with room for improvement for {self.current_grade}. Practice the choreography sequence regularly,\n",
    "            focusing particularly on the positions marked \"Needs Improvement\" or \"Fair\".</p>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            html += f\"\"\"\n",
    "            <p>This choreography needs more practice to meet the {self.current_grade} standard. Start by focusing on each position individually\n",
    "            before attempting the full sequence. Use the demonstration video as a reference and\n",
    "            practice in front of a mirror.</p>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add areas for improvement\n",
    "        html += \"<h3>Priority Areas for Improvement</h3><ul>\"\n",
    "        \n",
    "        # Sort results by similarity (ascending) to find weakest areas\n",
    "        sorted_results = sorted(results, key=lambda x: x['overall_similarity'])\n",
    "        for result in sorted_results[:3]:  # Top 3 weakest areas\n",
    "            html += f\"<li><strong>{result['label']}</strong> ({result['overall_similarity']:.1f}%)</li>\"\n",
    "        \n",
    "        html += \"</ul>\"\n",
    "        \n",
    "        # Add encouragement\n",
    "        html += f\"\"\"\n",
    "        <p style=\"margin-top: 20px;\">\n",
    "            Remember that consistent practice is key to mastering Spanish dance techniques at the {self.current_grade} level.\n",
    "            Focus on one improvement area at a time for the best results.\n",
    "        </p>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def visualize_comparison(self, performance_analysis, output_path=None):\n",
    "        \"\"\"Create a visual comparison of reference and student poses\"\"\"\n",
    "        if not performance_analysis or not performance_analysis['matching_results']:\n",
    "            return None\n",
    "        \n",
    "        results = performance_analysis['matching_results']\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        rows = len(results)\n",
    "        fig, axs = plt.subplots(rows, 2, figsize=(12, 4 * rows))\n",
    "        \n",
    "        if rows == 1:\n",
    "            axs = [axs]  # Make it iterable for single row\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            \n",
    "            # Find reference pose\n",
    "            ref_pose_idx = result['reference_pose_idx']\n",
    "            ref_pose = self.reference_poses[ref_pose_idx]\n",
    "            \n",
    "            # Find the nearest keyframe\n",
    "            ref_keyframe = None\n",
    "            min_dist = float('inf')\n",
    "            for kf in self.reference_keyframes:\n",
    "                dist = abs(kf['frame_idx'] - ref_pose['frame_idx'])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    ref_keyframe = kf\n",
    "            \n",
    "            if ref_keyframe:\n",
    "                ref_frame = ref_keyframe['frame']\n",
    "                \n",
    "                # Find student pose\n",
    "                student_pose_idx = result['student_pose_idx']\n",
    "                if student_pose_idx >= 0:\n",
    "                    # We would need to have saved student frames, which we don't for memory reasons\n",
    "                    # Placeholder for actual student frame visualization\n",
    "                    student_frame = np.zeros_like(ref_frame)\n",
    "                    \n",
    "                    # Draw pose comparison\n",
    "                    axs[i][0].imshow(cv2.cvtColor(ref_frame, cv2.COLOR_BGR2RGB))\n",
    "                    axs[i][0].set_title(f\"Reference: {label}\")\n",
    "                    axs[i][0].axis('off')\n",
    "                    \n",
    "                    axs[i][1].imshow(cv2.cvtColor(student_frame, cv2.COLOR_BGR2RGB))\n",
    "                    axs[i][1].set_title(f\"Student: {similarity:.1f}% similarity\")\n",
    "                    axs[i][1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            return output_path\n",
    "        else:\n",
    "            plt.show()\n",
    "            return fig\n",
    "\n",
    "\n",
    "def load_position_keypoints_from_roboflow(analyzer, api_key):\n",
    "    \"\"\"Helper function to load keypoints - implementation placeholder\"\"\"\n",
    "    # This is a simplified version - you would need to implement the actual \n",
    "    # Roboflow API calls to retrieve keypoint data for each position\n",
    "    print(\"Loading position keypoints from Roboflow...\")\n",
    "    \n",
    "    # Mock implementation - replace with actual Roboflow API calls\n",
    "    positions = [f\"position{i}\" for i in range(1, 16)]\n",
    "    for position in positions:\n",
    "        # In a real implementation, you would fetch actual keypoint data from Roboflow\n",
    "        analyzer.position_keypoints[position] = []\n",
    "    \n",
    "    # Return True if successful, False otherwise\n",
    "    return len(analyzer.position_keypoints) > 0\n",
    "\n",
    "\n",
    "# GUI Application for the Spanish Dance Analysis Tool\n",
    "class SpanishDanceApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Spanish Dance Analysis Tool\")\n",
    "        self.root.geometry(\"1000x700\")\n",
    "        \n",
    "        # Initialize the analyzer with grade information\n",
    "        self.analyzer = SpanishDanceAnalyzer()\n",
    "        \n",
    "        # Try to load existing data\n",
    "        self.analyzer.load_reference_data()\n",
    "        self.analyzer.load_choreography_sequence()\n",
    "        \n",
    "        # Create the main interface\n",
    "        self.create_interface()\n",
    "        \n",
    "        # Status variables\n",
    "        self.demo_video_path = None\n",
    "        self.student_video_path = None\n",
    "        self.performance_results = None\n",
    "        \n",
    "        # Video preview variables\n",
    "        self.preview_active = False\n",
    "        self.demo_cap = None\n",
    "        self.student_cap = None\n",
    "        \n",
    "    def toggle_keypoint_comparison(self):\n",
    "        \"\"\"Toggle whether to use Roboflow keypoints for comparison\"\"\"\n",
    "        use_keypoints = self.use_keypoints_var.get()\n",
    "        if hasattr(self, 'analyzer'):\n",
    "            self.analyzer.use_keypoint_comparison = use_keypoints\n",
    "            \n",
    "            status = \"enabled\" if use_keypoints else \"disabled\"\n",
    "            self.status_var.set(f\"Keypoint comparison {status}\")\n",
    "            \n",
    "            # If keypoints are enabled but none are loaded, remind the user\n",
    "            if use_keypoints and hasattr(self.analyzer, 'position_keypoints') and not self.analyzer.position_keypoints:\n",
    "                messagebox.showinfo(\"Keypoints Required\", \n",
    "                                \"Please set your Roboflow API key to load position keypoints\")\n",
    "                \n",
    "        else:\n",
    "            self.status_var.set(\"Analyzer not initialized\")\n",
    "            self.use_keypoints_var.set(False)\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the application interface\"\"\"\n",
    "        # Create a notebook with tabs\n",
    "        notebook = ttk.Notebook(self.root)\n",
    "        notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Tab 1: Setup and Configuration\n",
    "        setup_tab = ttk.Frame(notebook)\n",
    "        notebook.add(setup_tab, text=\"Setup\")\n",
    "        self.create_setup_tab(setup_tab)\n",
    "        \n",
    "        # Tab 2: Student Evaluation\n",
    "        evaluation_tab = ttk.Frame(notebook)\n",
    "        notebook.add(evaluation_tab, text=\"Evaluation\")\n",
    "        self.create_evaluation_tab(evaluation_tab)\n",
    "        \n",
    "        # Tab 3: Results and Reports\n",
    "        results_tab = ttk.Frame(notebook)\n",
    "        notebook.add(results_tab, text=\"Results\")\n",
    "        self.create_results_tab(results_tab)\n",
    "        \n",
    "        # Status bar\n",
    "        status_frame = ttk.Frame(self.root)\n",
    "        status_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        status_label = ttk.Label(status_frame, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        status_label.pack(fill=tk.X)\n",
    "    \n",
    "    def create_setup_tab(self, parent):\n",
    "        \"\"\"Create the setup tab interface with improved choreography management\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Demo Video Section\n",
    "        demo_frame = ttk.LabelFrame(frame, text=\"Step 1: Set Demonstration Video\", padding=10)\n",
    "        demo_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(demo_frame, text=\"Select a video showing the correct dance choreography:\").pack(anchor=tk.W)\n",
    "        \n",
    "        btn_frame = ttk.Frame(demo_frame)\n",
    "        btn_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(btn_frame, text=\"Load Demo Video\", command=self.load_demo_video).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.demo_status_var = tk.StringVar()\n",
    "        self.demo_status_var.set(\"No demo video loaded\")\n",
    "        ttk.Label(demo_frame, textvariable=self.demo_status_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # Add keypoint comparison option after the demo video section\n",
    "        keypoint_frame = ttk.Frame(demo_frame)\n",
    "        keypoint_frame.pack(fill=tk.X, pady=5)\n",
    "\n",
    "        self.use_keypoints_var = tk.BooleanVar(value=True)\n",
    "        ttk.Checkbutton(keypoint_frame, text=\"Use Roboflow keypoints for comparison\", \n",
    "                    variable=self.use_keypoints_var, \n",
    "                    command=self.toggle_keypoint_comparison).pack(anchor=tk.W)\n",
    "        \n",
    "        # 2. Choreography Definition Section\n",
    "        choreo_frame = ttk.LabelFrame(frame, text=\"Step 2: Define Choreography Sequence\", padding=10)\n",
    "        choreo_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(choreo_frame, text=\"After loading the demo video, define key dance positions/steps:\").pack(anchor=tk.W)\n",
    "        \n",
    "        # Add a description about timestamps\n",
    "        ttk.Label(choreo_frame, \n",
    "                text=\"Timestamps help identify when each position occurs in the demo video.\",\n",
    "                font=('TkDefaultFont', 9, 'italic')).pack(anchor=tk.W, pady=(0, 5))\n",
    "        \n",
    "        # Treeview with columns for timestamp and position\n",
    "        self.choreo_list = ttk.Treeview(choreo_frame, columns=(\"Timestamp\", \"Position\"), show=\"headings\", height=6)\n",
    "        self.choreo_list.heading(\"Timestamp\", text=\"Time (seconds)\")\n",
    "        self.choreo_list.heading(\"Position\", text=\"Position/Step Name\")\n",
    "        self.choreo_list.column(\"Timestamp\", width=100)\n",
    "        self.choreo_list.column(\"Position\", width=300)\n",
    "        self.choreo_list.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        # Add a scrollbar\n",
    "        choreo_scroll = ttk.Scrollbar(choreo_frame, orient=\"vertical\", command=self.choreo_list.yview)\n",
    "        choreo_scroll.place(relx=1.0, rely=0.5, relheight=0.4, anchor=\"e\")\n",
    "        self.choreo_list.configure(yscrollcommand=choreo_scroll.set)\n",
    "        \n",
    "        choreo_buttons = ttk.Frame(choreo_frame)\n",
    "        choreo_buttons.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(choreo_buttons, text=\"Add Position\", command=self.add_choreography_point).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(choreo_buttons, text=\"Remove Selected\", command=self.remove_choreography_point).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(choreo_buttons, text=\"Save Sequence\", command=self.save_choreography).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(choreo_buttons, text=\"Load Saved Sequence\", command=self.load_saved_choreography).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # 3. Configuration Section\n",
    "        config_frame = ttk.LabelFrame(frame, text=\"Step 3: Configuration\", padding=10)\n",
    "        config_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(config_frame, text=\"Add your Roboflow API key (required for keypoint comparison):\").pack(anchor=tk.W)\n",
    "        \n",
    "        api_frame = ttk.Frame(config_frame)\n",
    "        api_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.api_key_var = tk.StringVar()\n",
    "        api_entry = ttk.Entry(api_frame, textvariable=self.api_key_var, width=40)\n",
    "        api_entry.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        ttk.Button(api_frame, text=\"Set API Key\", command=self.set_api_key).pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    def create_evaluation_tab(self, parent):\n",
    "        \"\"\"Create the evaluation tab interface\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Student Video Section\n",
    "        student_frame = ttk.LabelFrame(frame, text=\"Step 1: Load Student Performance\", padding=10)\n",
    "        student_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(student_frame, text=\"Select a video of the student performing the choreography:\").pack(anchor=tk.W)\n",
    "        \n",
    "        btn_frame = ttk.Frame(student_frame)\n",
    "        btn_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Button(btn_frame, text=\"Load Student Video\", command=self.load_student_video).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.student_status_var = tk.StringVar()\n",
    "        self.student_status_var.set(\"No student video loaded\")\n",
    "        ttk.Label(student_frame, textvariable=self.student_status_var).pack(anchor=tk.W, pady=5)\n",
    "\n",
    "        # 2: Grade Selection Section\n",
    "        grade_frame = ttk.LabelFrame(frame, text=\"Step 2: Select Student Grade\", padding=10)\n",
    "        grade_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(grade_frame, text=\"Select the student's grade level:\").pack(anchor=tk.W)\n",
    "        \n",
    "        self.grade_var = tk.StringVar()\n",
    "        self.grade_var.set(\"Grado Uno\")  # Default grade\n",
    "        \n",
    "        grade_combo = ttk.Combobox(grade_frame, textvariable=self.grade_var, \n",
    "                                  values=GradeThresholds.get_grade_names(),\n",
    "                                  state=\"readonly\", width=30)\n",
    "        grade_combo.pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        ttk.Button(grade_frame, text=\"Set Grade\", command=self.set_student_grade).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        self.grade_info_var = tk.StringVar()\n",
    "        self.grade_info_var.set(\"Current Grade: Grado Uno\")\n",
    "        ttk.Label(grade_frame, textvariable=self.grade_info_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # Show thresholds for selected grade\n",
    "        thresholds_frame = ttk.Frame(grade_frame)\n",
    "        thresholds_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.threshold_vars = {\n",
    "            \"excellent\": tk.StringVar(value=\"Excellent: ≥75%\"),\n",
    "            \"good\": tk.StringVar(value=\"Good: ≥65%\"),\n",
    "            \"fair\": tk.StringVar(value=\"Fair: ≥55%\")\n",
    "        }\n",
    "        \n",
    "        for level, var in self.threshold_vars.items():\n",
    "            ttk.Label(thresholds_frame, textvariable=var).pack(anchor=tk.W)\n",
    "        \n",
    "        # Update thresholds display when grade changes\n",
    "        grade_combo.bind(\"<<ComboboxSelected>>\", self.update_threshold_display)\n",
    "        \n",
    "        # 3. Analysis Section\n",
    "        analysis_frame = ttk.LabelFrame(frame, text=\"Step 3: Analyze Performance\", padding=10)\n",
    "        analysis_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(analysis_frame, text=\"Compare the student's performance with the demonstration:\").pack(anchor=tk.W)\n",
    "        \n",
    "        ttk.Button(analysis_frame, text=\"Analyze Performance\", command=self.analyze_performance).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        self.analysis_progress = ttk.Progressbar(analysis_frame, orient=tk.HORIZONTAL, mode='indeterminate')\n",
    "        self.analysis_progress.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        self.analysis_status_var = tk.StringVar()\n",
    "        self.analysis_status_var.set(\"Ready for analysis\")\n",
    "        ttk.Label(analysis_frame, textvariable=self.analysis_status_var).pack(anchor=tk.W, pady=5)\n",
    "        \n",
    "        # 4. Preview Section\n",
    "        preview_frame = ttk.LabelFrame(frame, text=\"Preview\", padding=10)\n",
    "        preview_frame.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        preview_container = ttk.Frame(preview_frame)\n",
    "        preview_container.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.demo_preview = ttk.LabelFrame(preview_container, text=\"Demonstration\")\n",
    "        self.demo_preview.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.demo_canvas = tk.Canvas(self.demo_preview, bg='black')\n",
    "        self.demo_canvas.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.student_preview = ttk.LabelFrame(preview_container, text=\"Student Performance\")\n",
    "        self.student_preview.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
    "        \n",
    "        self.student_canvas = tk.Canvas(self.student_preview, bg='black')\n",
    "        self.student_canvas.pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    def create_results_tab(self, parent):\n",
    "        \"\"\"Create the results tab interface\"\"\"\n",
    "        frame = ttk.Frame(parent, padding=10)\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # 1. Summary Section\n",
    "        summary_frame = ttk.LabelFrame(frame, text=\"Performance Summary\", padding=10)\n",
    "        summary_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        summary_grid = ttk.Frame(summary_frame)\n",
    "        summary_grid.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Overall Technique Mark:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.overall_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.overall_score_var, font=('TkDefaultFont', 12, 'bold')).grid(row=0, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Arm Movements:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.arm_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.arm_score_var).grid(row=1, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(summary_grid, text=\"Footwork:\").grid(row=2, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.foot_score_var = tk.StringVar()\n",
    "        ttk.Label(summary_grid, textvariable=self.foot_score_var).grid(row=2, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        # 2. Detailed Results Section\n",
    "        results_frame = ttk.LabelFrame(frame, text=\"Detailed Results\", padding=10)\n",
    "        results_frame.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        # Treeview for results table\n",
    "        self.results_tree = ttk.Treeview(results_frame, columns=(\"Position\", \"Execution\", \"Similarity\", \"Feedback\"), show=\"headings\", height=10)\n",
    "        self.results_tree.heading(\"Position\", text=\"Position/Step\")\n",
    "        self.results_tree.heading(\"Execution\", text=\"Execution\")\n",
    "        self.results_tree.heading(\"Similarity\", text=\"Similarity\")\n",
    "        self.results_tree.heading(\"Feedback\", text=\"Feedback\")\n",
    "        \n",
    "        self.results_tree.column(\"Position\", width=150)\n",
    "        self.results_tree.column(\"Execution\", width=100)\n",
    "        self.results_tree.column(\"Similarity\", width=80)\n",
    "        self.results_tree.column(\"Feedback\", width=400)\n",
    "        \n",
    "        self.results_tree.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        \n",
    "        # 3. Export Section\n",
    "        export_frame = ttk.Frame(frame)\n",
    "        export_frame.pack(fill=tk.X, pady=10)\n",
    "        \n",
    "        ttk.Button(export_frame, text=\"Generate Detailed Report\", command=self.generate_report).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(export_frame, text=\"Export Results\", command=self.export_results).pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    def add_choreography_point(self):\n",
    "        \"\"\"Add a choreography point to the sequence with timestamp selection\"\"\"\n",
    "        if not self.demo_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a demonstration video first\")\n",
    "            return\n",
    "        \n",
    "        # Create a more comprehensive dialog for adding choreography points\n",
    "        dialog = tk.Toplevel(self.root)\n",
    "        dialog.title(\"Add Choreography Point\")\n",
    "        dialog.geometry(\"400x220\")\n",
    "        dialog.resizable(False, False)\n",
    "        \n",
    "        # Position Name\n",
    "        ttk.Label(dialog, text=\"Position/Step Name:\", font=('TkDefaultFont', 10, 'bold')).pack(anchor=tk.W, padx=10, pady=5)\n",
    "        label_var = tk.StringVar()\n",
    "        position_frame = ttk.Frame(dialog)\n",
    "        position_frame.pack(fill=tk.X, padx=10, pady=2)\n",
    "        \n",
    "        positions = [f\"position{i}\" for i in range(1, 16)]\n",
    "        position_combo = ttk.Combobox(position_frame, textvariable=label_var, values=positions, width=30)\n",
    "        position_combo.pack(side=tk.LEFT)\n",
    "        \n",
    "        # Timestamp Selection\n",
    "        ttk.Label(dialog, text=\"Timestamp (seconds):\", font=('TkDefaultFont', 10, 'bold')).pack(anchor=tk.W, padx=10, pady=5)\n",
    "        \n",
    "        timestamp_frame = ttk.Frame(dialog)\n",
    "        timestamp_frame.pack(fill=tk.X, padx=10, pady=2)\n",
    "        \n",
    "        timestamp_var = tk.DoubleVar(value=0.0)\n",
    "        timestamp_entry = ttk.Entry(timestamp_frame, textvariable=timestamp_var, width=10)\n",
    "        timestamp_entry.pack(side=tk.LEFT)\n",
    "        \n",
    "        # If we have reference keyframes, show them for easy timestamp selection\n",
    "        if hasattr(self.analyzer, 'reference_keyframes') and self.analyzer.reference_keyframes:\n",
    "            ttk.Label(timestamp_frame, text=\"or\").pack(side=tk.LEFT, padx=5)\n",
    "            \n",
    "            # Calculate timestamp values from reference keyframes\n",
    "            timestamps = [kf['timestamp'] for kf in self.analyzer.reference_keyframes]\n",
    "            timestamp_combo = ttk.Combobox(timestamp_frame, width=15)\n",
    "            timestamp_combo['values'] = [f\"{ts:.2f}\" for ts in timestamps]\n",
    "            timestamp_combo.pack(side=tk.LEFT, padx=5)\n",
    "            \n",
    "            # When a timestamp is selected from dropdown, update the entry\n",
    "            def on_timestamp_select(event):\n",
    "                selected = timestamp_combo.get()\n",
    "                if selected:\n",
    "                    timestamp_var.set(float(selected))\n",
    "            \n",
    "            timestamp_combo.bind(\"<<ComboboxSelected>>\", on_timestamp_select)\n",
    "            \n",
    "            # Add a preview option\n",
    "            ttk.Label(dialog, text=\"Preview:\", font=('TkDefaultFont', 10)).pack(anchor=tk.W, padx=10, pady=2)\n",
    "            preview_frame = ttk.Frame(dialog)\n",
    "            preview_frame.pack(fill=tk.X, padx=10, pady=2)\n",
    "            \n",
    "            preview_var = tk.BooleanVar(value=False)\n",
    "            preview_check = ttk.Checkbutton(preview_frame, text=\"Show keyframe when selecting timestamp\", \n",
    "                                         variable=preview_var, command=lambda: self.toggle_preview(preview_var.get(), timestamp_combo))\n",
    "            preview_check.pack(anchor=tk.W)\n",
    "        \n",
    "        # Button Frame\n",
    "        button_frame = ttk.Frame(dialog)\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "        \n",
    "        def add_and_close():\n",
    "            position = label_var.get()\n",
    "            timestamp = timestamp_var.get()\n",
    "            \n",
    "            if not position:\n",
    "                messagebox.showwarning(\"Warning\", \"Please enter a valid position name\")\n",
    "                return\n",
    "            \n",
    "            if position not in positions:\n",
    "                messagebox.showwarning(\"Warning\", \n",
    "                    f\"Position '{position}' is not recognized.\\nPlease use position1 through position15.\")\n",
    "                return\n",
    "            \n",
    "            # Add to sequence with proper timestamp\n",
    "            self.choreo_list.insert(\"\", \"end\", values=(f\"{timestamp:.2f}\", position))\n",
    "            dialog.destroy()\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"Add\", command=add_and_close).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(button_frame, text=\"Cancel\", command=dialog.destroy).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Set initial focus to position combo\n",
    "        position_combo.focus_set()\n",
    "        \n",
    "        # Set dialog modal\n",
    "        dialog.transient(self.root)\n",
    "        dialog.grab_set()\n",
    "        self.root.wait_window(dialog)\n",
    "\n",
    "    def toggle_preview(self, show_preview, timestamp_combo):\n",
    "        \"\"\"Toggle preview of keyframe when selecting timestamps\"\"\"\n",
    "        if not show_preview:\n",
    "            if hasattr(self, 'preview_window') and self.preview_window.winfo_exists():\n",
    "                self.preview_window.destroy()\n",
    "            return\n",
    "        \n",
    "        # Get the selected timestamp\n",
    "        try:\n",
    "            selected = timestamp_combo.get()\n",
    "            if not selected:\n",
    "                return\n",
    "                \n",
    "            ts = float(selected)\n",
    "            \n",
    "            # Find the keyframe closest to this timestamp\n",
    "            keyframe = None\n",
    "            for kf in self.analyzer.reference_keyframes:\n",
    "                if abs(kf['timestamp'] - ts) < 0.1:  # Within 0.1 second\n",
    "                    keyframe = kf\n",
    "                    break\n",
    "            \n",
    "            if keyframe and 'frame' in keyframe:\n",
    "                # If we have a preview window, update it\n",
    "                if hasattr(self, 'preview_window') and self.preview_window.winfo_exists():\n",
    "                    # Update the image\n",
    "                    preview_img = self.convert_cv_to_tk(keyframe['frame'])\n",
    "                    self.preview_label.configure(image=preview_img)\n",
    "                    self.preview_label.image = preview_img\n",
    "                else:\n",
    "                    # Create a preview window\n",
    "                    self.preview_window = tk.Toplevel(self.root)\n",
    "                    self.preview_window.title(f\"Keyframe at {ts:.2f}s\")\n",
    "                    self.preview_window.geometry(\"400x400\")\n",
    "                    \n",
    "                    preview_img = self.convert_cv_to_tk(keyframe['frame'])\n",
    "                    self.preview_label = ttk.Label(self.preview_window, image=preview_img)\n",
    "                    self.preview_label.image = preview_img  # Keep a reference\n",
    "                    self.preview_label.pack(padx=5, pady=5)\n",
    "                    \n",
    "                    # When selecting a different timestamp, update the preview\n",
    "                    timestamp_combo.bind(\"<<ComboboxSelected>>\", \n",
    "                                       lambda event: self.update_preview(timestamp_combo))\n",
    "                    \n",
    "                    # When closing the main dialog, close the preview too\n",
    "                    self.preview_window.protocol(\"WM_DELETE_WINDOW\", \n",
    "                                              lambda: self.preview_window.destroy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error showing preview: {e}\")\n",
    "\n",
    "    def update_preview(self, timestamp_combo):\n",
    "        \"\"\"Update the preview window with a new keyframe\"\"\"\n",
    "        if not hasattr(self, 'preview_window') or not self.preview_window.winfo_exists():\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            selected = timestamp_combo.get()\n",
    "            if not selected:\n",
    "                return\n",
    "                \n",
    "            ts = float(selected)\n",
    "            \n",
    "            # Find the keyframe closest to this timestamp\n",
    "            keyframe = None\n",
    "            for kf in self.analyzer.reference_keyframes:\n",
    "                if abs(kf['timestamp'] - ts) < 0.1:  # Within 0.1 second\n",
    "                    keyframe = kf\n",
    "                    break\n",
    "            \n",
    "            if keyframe and 'frame' in keyframe:\n",
    "                # Update the preview image\n",
    "                preview_img = self.convert_cv_to_tk(keyframe['frame'])\n",
    "                self.preview_label.configure(image=preview_img)\n",
    "                self.preview_label.image = preview_img\n",
    "                self.preview_window.title(f\"Keyframe at {ts:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating preview: {e}\")\n",
    "    \n",
    "    def start_video_preview(self):\n",
    "        \"\"\"Start video preview with MediaPipe visualization\"\"\"\n",
    "        self.preview_active = True\n",
    "        \n",
    "        # Initialize video captures\n",
    "        self.demo_cap = cv2.VideoCapture(self.demo_video_path)\n",
    "        self.student_cap = cv2.VideoCapture(self.student_video_path)\n",
    "        \n",
    "        if not self.demo_cap.isOpened() or not self.student_cap.isOpened():\n",
    "            messagebox.showerror(\"Error\", \"Failed to open video files\")\n",
    "            self.preview_active = False\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        demo_fps = self.demo_cap.get(cv2.CAP_PROP_FPS)\n",
    "        student_fps = self.student_cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Use the lower FPS for synchronization\n",
    "        target_fps = min(demo_fps, student_fps)\n",
    "        frame_delay = int(1000 / target_fps) if target_fps > 0 else 33\n",
    "        \n",
    "        # Start preview update loop\n",
    "        self.update_preview_videos(frame_delay)\n",
    "    \n",
    "    def update_preview_videos(self, frame_delay):\n",
    "        \"\"\"Update preview frames with MediaPipe visualization\"\"\"\n",
    "        if not self.preview_active:\n",
    "            return\n",
    "        \n",
    "        # Read frames from both videos\n",
    "        ret_demo, demo_frame = self.demo_cap.read()\n",
    "        ret_student, student_frame = self.student_cap.read()\n",
    "        \n",
    "        # Check if we've reached the end of either video\n",
    "        if not ret_demo or not ret_student:\n",
    "            # Loop videos\n",
    "            self.demo_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            self.student_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            ret_demo, demo_frame = self.demo_cap.read()\n",
    "            ret_student, student_frame = self.student_cap.read()\n",
    "        \n",
    "        if ret_demo and ret_student:\n",
    "            # Process demo frame with MediaPipe\n",
    "            demo_rgb = cv2.cvtColor(demo_frame, cv2.COLOR_BGR2RGB)\n",
    "            demo_results = self.analyzer.pose.process(demo_rgb)\n",
    "            \n",
    "            if demo_results.pose_landmarks:\n",
    "                # Draw landmarks on demo frame\n",
    "                self.analyzer.mp_drawing.draw_landmarks(\n",
    "                    demo_frame, \n",
    "                    demo_results.pose_landmarks, \n",
    "                    self.analyzer.mp_pose.POSE_CONNECTIONS,\n",
    "                    self.analyzer.mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    self.analyzer.mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "                \n",
    "                # Draw enhanced foot landmarks\n",
    "                landmarks = demo_results.pose_landmarks.landmark\n",
    "                h, w, c = demo_frame.shape\n",
    "                \n",
    "                foot_landmarks = [\n",
    "                    (31, \"Left Foot\"), (32, \"Right Foot\"),\n",
    "                    (29, \"Left Heel\"), (30, \"Right Heel\"),\n",
    "                    (27, \"Left Ankle\"), (28, \"Right Ankle\")\n",
    "                ]\n",
    "                \n",
    "                for idx, name in foot_landmarks:\n",
    "                    if landmarks[idx].visibility > 0.5:\n",
    "                        cx, cy = int(landmarks[idx].x * w), int(landmarks[idx].y * h)\n",
    "                        cv2.circle(demo_frame, (cx, cy), 8, (0, 255, 0), -1)\n",
    "            \n",
    "            # Process student frame with MediaPipe\n",
    "            student_rgb = cv2.cvtColor(student_frame, cv2.COLOR_BGR2RGB)\n",
    "            student_results = self.analyzer.pose.process(student_rgb)\n",
    "            \n",
    "            if student_results.pose_landmarks:\n",
    "                # Draw landmarks on student frame\n",
    "                self.analyzer.mp_drawing.draw_landmarks(\n",
    "                    student_frame, \n",
    "                    student_results.pose_landmarks, \n",
    "                    self.analyzer.mp_pose.POSE_CONNECTIONS,\n",
    "                    self.analyzer.mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    self.analyzer.mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "                \n",
    "                # Draw enhanced foot landmarks\n",
    "                landmarks = student_results.pose_landmarks.landmark\n",
    "                h, w, c = student_frame.shape\n",
    "                \n",
    "                for idx, name in foot_landmarks:\n",
    "                    if landmarks[idx].visibility > 0.5:\n",
    "                        cx, cy = int(landmarks[idx].x * w), int(landmarks[idx].y * h)\n",
    "                        cv2.circle(student_frame, (cx, cy), 8, (0, 255, 0), -1)\n",
    "            \n",
    "            # Add labels to frames\n",
    "            cv2.putText(demo_frame, 'Demonstration', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(student_frame, 'Student Performance', (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display frames in canvases\n",
    "            self.display_frame_on_canvas(demo_frame, self.demo_canvas)\n",
    "            self.display_frame_on_canvas(student_frame, self.student_canvas)\n",
    "        \n",
    "        # Schedule next update\n",
    "        if self.preview_active:\n",
    "            self.root.after(frame_delay, lambda: self.update_preview_videos(frame_delay))\n",
    "    \n",
    "    def display_frame_on_canvas(self, frame, canvas):\n",
    "        \"\"\"Display a frame on a tkinter canvas\"\"\"\n",
    "        # Get canvas dimensions\n",
    "        canvas_width = canvas.winfo_width()\n",
    "        canvas_height = canvas.winfo_height()\n",
    "        \n",
    "        if canvas_width <= 1 or canvas_height <= 1:\n",
    "            # Canvas not yet properly initialized\n",
    "            canvas_width = 400\n",
    "            canvas_height = 300\n",
    "        \n",
    "        # Resize frame to fit canvas while maintaining aspect ratio\n",
    "        h, w = frame.shape[:2]\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if canvas_width / canvas_height > aspect_ratio:\n",
    "            # Canvas is wider than frame aspect ratio\n",
    "            new_height = canvas_height\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "        else:\n",
    "            # Canvas is taller than frame aspect ratio\n",
    "            new_width = canvas_width\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        \n",
    "        # Resize frame\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Convert to RGB and create PhotoImage\n",
    "        rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "        photo = ImageTk.PhotoImage(image=img)\n",
    "        \n",
    "        # Clear canvas and display image\n",
    "        canvas.delete(\"all\")\n",
    "        x = (canvas_width - new_width) // 2\n",
    "        y = (canvas_height - new_height) // 2\n",
    "        canvas.create_image(x, y, anchor=tk.NW, image=photo)\n",
    "        \n",
    "        # Keep a reference to prevent garbage collection\n",
    "        canvas.image = photo\n",
    "    \n",
    "    def stop_video_preview(self):\n",
    "        \"\"\"Stop video preview\"\"\"\n",
    "        self.preview_active = False\n",
    "        \n",
    "        if self.demo_cap:\n",
    "            self.demo_cap.release()\n",
    "            self.demo_cap = None\n",
    "        \n",
    "        if self.student_cap:\n",
    "            self.student_cap.release()\n",
    "            self.student_cap = None\n",
    "        \n",
    "        # Clear canvases\n",
    "        self.demo_canvas.delete(\"all\")\n",
    "        self.student_canvas.delete(\"all\")\n",
    "    \n",
    "    def convert_cv_to_tk(self, cv_image):\n",
    "        \"\"\"Convert OpenCV BGR image to Tkinter PhotoImage\"\"\"\n",
    "        # Resize if too large\n",
    "        height, width = cv_image.shape[:2]\n",
    "        max_size = 400\n",
    "        if height > max_size or width > max_size:\n",
    "            scale = max_size / max(height, width)\n",
    "            cv_image = cv2.resize(cv_image, None, fx=scale, fy=scale)\n",
    "        \n",
    "        # Convert to RGB and then to PhotoImage\n",
    "        rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(rgb_image)\n",
    "        return ImageTk.PhotoImage(pil_image)\n",
    "    \n",
    "    def remove_choreography_point(self):\n",
    "        \"\"\"Remove selected choreography point\"\"\"\n",
    "        selected = self.choreo_list.selection()\n",
    "        if selected:\n",
    "            self.choreo_list.delete(selected)\n",
    "    \n",
    "    def save_choreography(self):\n",
    "        \"\"\"Save the defined choreography sequence with actual timestamps\"\"\"\n",
    "        items = self.choreo_list.get_children()\n",
    "        if not items:\n",
    "            messagebox.showwarning(\"Warning\", \"No choreography points defined\")\n",
    "            return\n",
    "        \n",
    "        # Extract labels and timestamps\n",
    "        labels = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for item in items:\n",
    "            values = self.choreo_list.item(item, \"values\")\n",
    "            timestamp_str, label = values\n",
    "            \n",
    "            try:\n",
    "                # Convert timestamp string to float\n",
    "                timestamp = float(timestamp_str) if timestamp_str else 0.0\n",
    "                timestamps.append(timestamp)\n",
    "                labels.append(label)\n",
    "            except ValueError:\n",
    "                messagebox.showwarning(\"Warning\", f\"Invalid timestamp: {timestamp_str}\")\n",
    "                return\n",
    "        \n",
    "        # Validate position names\n",
    "        for label in labels:\n",
    "            if label not in [f\"position{i}\" for i in range(1, 16)]:\n",
    "                messagebox.showwarning(\"Warning\", \n",
    "                    f\"Position '{label}' is not recognized.\\nPlease use position1 through position15.\")\n",
    "                return\n",
    "        \n",
    "        # Save choreography with actual timestamps\n",
    "        count = self.analyzer.define_choreography_sequence(timestamps, labels)\n",
    "        \n",
    "        self.status_var.set(f\"Choreography sequence saved: {count} positions\")\n",
    "        messagebox.showinfo(\"Success\", f\"Choreography sequence with {count} positions saved successfully\")\n",
    "        \n",
    "        # Save to a separate file for reuse\n",
    "        self.save_choreography_for_reuse(timestamps, labels)\n",
    "\n",
    "    def save_choreography_for_reuse(self, timestamps, labels):\n",
    "        \"\"\"Save choreography sequence to a separate file for reuse with other students\"\"\"\n",
    "        choreo_sequence = []\n",
    "        for ts, label in zip(timestamps, labels):\n",
    "            choreo_sequence.append({\n",
    "                'timestamp': ts,\n",
    "                'label': label\n",
    "            })\n",
    "        \n",
    "        # Ask user for a name for this choreography\n",
    "        choreo_name = simpledialog.askstring(\"Save Choreography\", \n",
    "                                          \"Enter a name for this choreography sequence:\",\n",
    "                                          parent=self.root)\n",
    "        \n",
    "        if not choreo_name:\n",
    "            choreo_name = \"default_choreography\"\n",
    "        \n",
    "        # Sanitize the name for use as a filename\n",
    "        import re\n",
    "        safe_name = re.sub(r'[^\\w\\-_\\. ]', '_', choreo_name)\n",
    "        \n",
    "        # Save to a JSON file in the data directory\n",
    "        choreo_dir = os.path.join(self.analyzer.data_dir, \"choreographies\")\n",
    "        os.makedirs(choreo_dir, exist_ok=True)\n",
    "        \n",
    "        choreo_file = os.path.join(choreo_dir, f\"{safe_name}.json\")\n",
    "        \n",
    "        with open(choreo_file, \"w\") as f:\n",
    "            json.dump(choreo_sequence, f, indent=4)\n",
    "        \n",
    "        self.status_var.set(f\"Choreography '{choreo_name}' saved for reuse\")\n",
    "    \n",
    "    def load_saved_choreography(self):\n",
    "        \"\"\"Load a previously saved choreography sequence\"\"\"\n",
    "        if not self.demo_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a demonstration video first\")\n",
    "            return\n",
    "        \n",
    "        choreo_dir = os.path.join(self.analyzer.data_dir, \"choreographies\")\n",
    "        if not os.path.exists(choreo_dir):\n",
    "            messagebox.showinfo(\"Info\", \"No saved choreography sequences found\")\n",
    "            return\n",
    "        \n",
    "        # Get list of saved choreographies\n",
    "        choreo_files = [f for f in os.listdir(choreo_dir) if f.endswith('.json')]\n",
    "        \n",
    "        if not choreo_files:\n",
    "            messagebox.showinfo(\"Info\", \"No saved choreography sequences found\")\n",
    "            return\n",
    "        \n",
    "        # Show dialog to select a choreography\n",
    "        dialog = tk.Toplevel(self.root)\n",
    "        dialog.title(\"Load Choreography\")\n",
    "        dialog.geometry(\"400x300\")\n",
    "        dialog.resizable(False, False)\n",
    "        \n",
    "        ttk.Label(dialog, text=\"Select a saved choreography sequence:\", \n",
    "                 font=('TkDefaultFont', 10, 'bold')).pack(anchor=tk.W, padx=10, pady=10)\n",
    "        \n",
    "        # Create a listbox with scrollbar\n",
    "        frame = ttk.Frame(dialog)\n",
    "        frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        scrollbar = ttk.Scrollbar(frame)\n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        \n",
    "        choreo_list = tk.Listbox(frame, yscrollcommand=scrollbar.set, font=('TkDefaultFont', 10))\n",
    "        choreo_list.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        scrollbar.config(command=choreo_list.yview)\n",
    "        \n",
    "        # Add choreography names to the list\n",
    "        for f in choreo_files:\n",
    "            name = os.path.splitext(f)[0]\n",
    "            choreo_list.insert(tk.END, name)\n",
    "        \n",
    "        # Select the first item by default\n",
    "        if choreo_list.size() > 0:\n",
    "            choreo_list.selection_set(0)\n",
    "        \n",
    "        # Button Frame\n",
    "        button_frame = ttk.Frame(dialog)\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "        \n",
    "        def load_selected():\n",
    "            selected_idx = choreo_list.curselection()\n",
    "            if not selected_idx:\n",
    "                messagebox.showwarning(\"Warning\", \"Please select a choreography\")\n",
    "                return\n",
    "            \n",
    "            selected_name = choreo_list.get(selected_idx[0])\n",
    "            choreo_file = os.path.join(choreo_dir, f\"{selected_name}.json\")\n",
    "            \n",
    "            try:\n",
    "                with open(choreo_file, \"r\") as f:\n",
    "                    choreo_sequence = json.load(f)\n",
    "                \n",
    "                # Clear existing choreography list\n",
    "                for item in self.choreo_list.get_children():\n",
    "                    self.choreo_list.delete(item)\n",
    "                \n",
    "                # Add loaded choreography points to the list\n",
    "                for point in choreo_sequence:\n",
    "                    timestamp = point.get('timestamp', 0.0)\n",
    "                    label = point.get('label', '')\n",
    "                    self.choreo_list.insert(\"\", \"end\", values=(f\"{timestamp:.2f}\", label))\n",
    "                \n",
    "                # Extract timestamps and labels for analyzer\n",
    "                timestamps = [point.get('timestamp', 0.0) for point in choreo_sequence]\n",
    "                labels = [point.get('label', '') for point in choreo_sequence]\n",
    "                \n",
    "                # Update the analyzer's choreography sequence\n",
    "                count = self.analyzer.define_choreography_sequence(timestamps, labels)\n",
    "                \n",
    "                self.status_var.set(f\"Loaded choreography '{selected_name}' with {count} positions\")\n",
    "                dialog.destroy()\n",
    "                \n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"Failed to load choreography: {e}\")\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"Load\", command=load_selected).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(button_frame, text=\"Cancel\", command=dialog.destroy).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Set dialog modal\n",
    "        dialog.transient(self.root)\n",
    "        dialog.grab_set()\n",
    "        self.root.wait_window(dialog)\n",
    "    \n",
    "    def load_demo_video(self):\n",
    "        \"\"\"Load and process demonstration video\"\"\"\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            title=\"Select Demonstration Video\",\n",
    "            filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        self.demo_video_path = filepath\n",
    "        self.demo_status_var.set(f\"Processing: {os.path.basename(filepath)}...\")\n",
    "        self.status_var.set(\"Processing demonstration video. This may take a while...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Process in a separate thread to not freeze the UI\n",
    "        def process_video():\n",
    "            count = self.analyzer.set_reference_demo(filepath)\n",
    "            \n",
    "            # Update UI in the main thread\n",
    "            self.root.after(0, lambda: self.demo_status_var.set(f\"Loaded: {os.path.basename(filepath)} - {count} poses extracted\"))\n",
    "            self.root.after(0, lambda: self.status_var.set(\"Demonstration video processed successfully\"))\n",
    "        \n",
    "        threading.Thread(target=process_video).start()\n",
    "    \n",
    "    def set_api_key(self):\n",
    "        \"\"\"Set Roboflow API key and load position keypoints\"\"\"\n",
    "        api_key = self.api_key_var.get().strip()\n",
    "        if not api_key:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a valid API key\")\n",
    "            return\n",
    "        \n",
    "        # Reinitialize analyzer with API key\n",
    "        self.analyzer = SpanishDanceAnalyzer(use_roboflow=True, roboflow_api_key=api_key)\n",
    "        \n",
    "        # Make sure to set the current grade\n",
    "        self.analyzer.set_grade(self.grade_var.get())\n",
    "        \n",
    "        # Reload data\n",
    "        self.analyzer.load_reference_data()\n",
    "        self.analyzer.load_choreography_sequence()\n",
    "        \n",
    "        # Set status to loading\n",
    "        self.status_var.set(\"Loading keypoints from Roboflow...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Enable use_keypoint_comparison flag (was originally True but may be lost in reinitialization)\n",
    "        self.analyzer.use_keypoint_comparison = True\n",
    "        \n",
    "        # Load keypoints in a separate thread\n",
    "        def load_keypoints():\n",
    "            # This is critical - we're using our fixed method\n",
    "            success = load_position_keypoints_from_roboflow(self.analyzer, api_key)\n",
    "            \n",
    "            # Update UI in main thread\n",
    "            if success:\n",
    "                self.root.after(0, lambda: self.status_var.set(f\"Loaded reference keypoints for positions\"))\n",
    "                self.root.after(0, lambda: messagebox.showinfo(\"Success\", \n",
    "                                                        \"Roboflow API key set and position keypoints loaded successfully\"))\n",
    "            else:\n",
    "                self.root.after(0, lambda: self.status_var.set(\"Failed to load position keypoints\"))\n",
    "                self.root.after(0, lambda: messagebox.showwarning(\"Warning\", \n",
    "                                                            \"API key set but could not load position keypoints\"))\n",
    "        \n",
    "        threading.Thread(target=load_keypoints).start()\n",
    "    \n",
    "    def load_student_video(self):\n",
    "        \"\"\"Load student performance video\"\"\"\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            title=\"Select Student Performance Video\",\n",
    "            filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        self.student_video_path = filepath\n",
    "        self.student_status_var.set(f\"Selected: {os.path.basename(filepath)}\")\n",
    "        self.status_var.set(f\"Student video selected: {os.path.basename(filepath)}\")\n",
    "    \n",
    "    def set_student_grade(self):\n",
    "        \"\"\"Set the student's grade level\"\"\"\n",
    "        grade = self.grade_var.get()\n",
    "        if self.analyzer.set_grade(grade):\n",
    "            self.grade_info_var.set(f\"Current Grade: {grade}\")\n",
    "            self.update_threshold_display()\n",
    "            self.status_var.set(f\"Grade set to {grade}\")\n",
    "    \n",
    "    def update_threshold_display(self, event=None):\n",
    "        \"\"\"Update the threshold display when grade changes\"\"\"\n",
    "        grade = self.grade_var.get()\n",
    "        thresholds = GradeThresholds.get_thresholds(grade)\n",
    "        \n",
    "        self.threshold_vars[\"excellent\"].set(f\"Excellent: ≥{thresholds['excellent']}%\")\n",
    "        self.threshold_vars[\"good\"].set(f\"Good: ≥{thresholds['good']}%\")\n",
    "        self.threshold_vars[\"fair\"].set(f\"Fair: ≥{thresholds['fair']}%\")\n",
    "    \n",
    "    def analyze_performance(self):\n",
    "        \"\"\"Analyze student performance compared to demonstration\"\"\"\n",
    "        if not self.demo_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a demonstration video first\")\n",
    "            return\n",
    "        \n",
    "        if not self.student_video_path:\n",
    "            messagebox.showwarning(\"Warning\", \"Please load a student performance video\")\n",
    "            return\n",
    "        \n",
    "        # Start progress bar\n",
    "        self.analysis_progress.start()\n",
    "        self.analysis_status_var.set(\"Analyzing performance... Please wait.\")\n",
    "        self.status_var.set(\"Analyzing student performance. This may take a while...\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Start video preview with MediaPipe\n",
    "        self.start_video_preview()\n",
    "        \n",
    "        # Process in a separate thread\n",
    "        def run_analysis():\n",
    "            # Analyze performance\n",
    "            results = self.analyzer.analyze_student_performance(self.student_video_path)\n",
    "            \n",
    "            # Update UI in main thread\n",
    "            self.root.after(0, lambda: self.update_results(results))\n",
    "        \n",
    "        threading.Thread(target=run_analysis).start()\n",
    "    \n",
    "    def update_results(self, results):\n",
    "        \"\"\"Update the UI with analysis results using grade-specific thresholds\"\"\"\n",
    "        # Stop progress bar\n",
    "        self.analysis_progress.stop()\n",
    "        \n",
    "        if not results:\n",
    "            self.analysis_status_var.set(\"Analysis failed. Please check logs.\")\n",
    "            self.status_var.set(\"Performance analysis failed\")\n",
    "            return\n",
    "        \n",
    "        self.performance_results = results\n",
    "        \n",
    "        # Update status\n",
    "        self.analysis_status_var.set(\"Analysis complete\")\n",
    "        self.status_var.set(\"Performance analysis completed successfully\")\n",
    "        \n",
    "        # Update scores in results tab\n",
    "        self.overall_score_var.set(f\"{results['overall_score']:.1f}%\")\n",
    "        self.arm_score_var.set(f\"{results['arm_score']:.1f}%\")\n",
    "        self.foot_score_var.set(f\"{results['foot_score']:.1f}%\")\n",
    "        \n",
    "        # Clear previous results\n",
    "        for i in self.results_tree.get_children():\n",
    "            self.results_tree.delete(i)\n",
    "        \n",
    "        # Get current grade thresholds\n",
    "        thresholds = self.analyzer.grade_thresholds\n",
    "        \n",
    "        # Add results to treeview with grade-specific evaluation\n",
    "        for result in results['matching_results']:\n",
    "            label = result['label']\n",
    "            similarity = result['overall_similarity']\n",
    "            \n",
    "            # Determine execution rating based on grade thresholds\n",
    "            if similarity >= thresholds[\"excellent\"]:\n",
    "                execution = \"Excellent\"\n",
    "            elif similarity >= thresholds[\"good\"]:\n",
    "                execution = \"Good\"\n",
    "            elif similarity >= thresholds[\"fair\"]:\n",
    "                execution = \"Fair\"\n",
    "            else:\n",
    "                execution = \"Needs Improvement\"\n",
    "            \n",
    "            # Generate grade-appropriate feedback\n",
    "            if similarity >= thresholds[\"excellent\"]:\n",
    "                feedback = f\"Excellent execution of {label}\"\n",
    "            elif similarity >= thresholds[\"good\"]:\n",
    "                feedback = f\"Good execution of {label} with minor adjustments needed\"\n",
    "            elif similarity >= thresholds[\"fair\"]:\n",
    "                feedback = f\"Fair execution of {label}, focus on form and technique\"\n",
    "            else:\n",
    "                feedback = f\"{label} needs significant improvement\"\n",
    "            \n",
    "            self.results_tree.insert(\"\", \"end\", values=(label, execution, f\"{similarity:.1f}%\", feedback))\n",
    "        \n",
    "        # Show completion message with grade level\n",
    "        messagebox.showinfo(\"Analysis Complete\", \n",
    "                          f\"Performance analysis complete for {self.analyzer.current_grade}.\\n\"\n",
    "                          f\"Overall score: {results['overall_score']:.1f}%\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate and display detailed performance report\"\"\"\n",
    "        if not self.performance_results:\n",
    "            messagebox.showwarning(\"Warning\", \"No analysis results available\")\n",
    "            return\n",
    "        \n",
    "        # Generate HTML report\n",
    "        html_report = self.analyzer.generate_feedback(self.performance_results)\n",
    "        \n",
    "        # Save HTML to temporary file\n",
    "        report_path = os.path.join(self.analyzer.data_dir, \"performance_report.html\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        # Open in default browser\n",
    "        import webbrowser\n",
    "        webbrowser.open(report_path)\n",
    "        \n",
    "        self.status_var.set(f\"Report generated and saved to {report_path}\")\n",
    "    \n",
    "    def export_results(self):\n",
    "        \"\"\"Export analysis results to a file\"\"\"\n",
    "        if not self.performance_results:\n",
    "            messagebox.showwarning(\"Warning\", \"No analysis results available\")\n",
    "            return\n",
    "        \n",
    "        filepath = filedialog.asksaveasfilename(\n",
    "            title=\"Save Results\",\n",
    "            defaultextension=\".json\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        # Create a serializable copy of results (removing non-JSON serializable objects)\n",
    "        results_copy = self.performance_results.copy()\n",
    "        \n",
    "        # Save to JSON\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(results_copy, f, indent=4)\n",
    "        \n",
    "        self.status_var.set(f\"Results exported to {filepath}\")\n",
    "        messagebox.showinfo(\"Success\", f\"Results exported successfully to {filepath}\")\n",
    "\n",
    "\n",
    "# Main entry point\n",
    "def main():\n",
    "    \"\"\"Main entry point of the application\"\"\"\n",
    "    root = tk.Tk()\n",
    "    app = SpanishDanceApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcvha-kernel",
   "language": "python",
   "name": "ipcvha-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
